{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ8-spwGrtMI"
      },
      "source": [
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/cabecera.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43dkXdozrtMK"
      },
      "source": [
        "## Una (muy) breve (pero densa) introducción a la IA Generativa en Textos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4Tab-WKrtMK"
      },
      "source": [
        "*NOTA: Este notebook adapta y amplía parte del capítulo dedicado a NLP en el excelente libro [Hands on Machine Learning for Python](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#nlp_chapter)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-TYG-VJrtML"
      },
      "source": [
        "La generación de textos y más que eso el tratamiento de lenguaje natural ha dado un salto \"cuántico\" en los últimos años dentro del campo de la IA. Vamos a ver de una forma poco ortodoxa la evolución partiendo de las arquitecturas más complejas sobre redes recurrentes hasta terminar en los instruct LLM (la base de la IA multimodal generativa actual). Será un viaje denso y breve, pero espero que despierte el interés en ti para ampliar más con el material extra que se proporciona en la plataforma y en algunos enlaces de este notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTwE2Qu4rtMM"
      },
      "source": [
        "*Antes de empezar, este notebook \"no\" se recomienda si no se ejecuta en un entorno con GPU disponible. Si no es el caso -> Colab:*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUIn-3_qrtMM"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/rodolso/DS_Online_Octubre24/blob/main/05_Deep_Learning/Sprint_19/Unidad_02_IA_Generativa_NLP_y_Texto/Generativa_Texto_De_RRN_a_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XXGCfC43rtMN"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rN3amKO8mBC"
      },
      "source": [
        "**Warning**: las versiones más recientes de TensorFlow están basadas en Keras 3. Para las sesiones previas no fue muy difícil actualizar el código para que fuera compatible con Keras 3, pero desafortunadamente es mucho más complicado para esta sesión (especialmente en la parte final de la misma). Por ello, nos vemos obligados a volver a Keras 2. Para hacer esto, vamos a configurar la variable de entorno `TF_USE_LEGACY_KERAS` a `\"1\"` e importamos el paquete `tf_keras`. Esto asegura que `tf.keras` apunte correctamente a `tf_keras`, que es Keras 2.*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "a0b9iwIEzGKD"
      },
      "outputs": [],
      "source": [
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if IS_COLAB:\n",
        "    import os\n",
        "    os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "    import tf_keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gCHANlswrtMO"
      },
      "outputs": [],
      "source": [
        "from packaging import version\n",
        "import sklearn\n",
        "\n",
        "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Rg025n4NrtMO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.10.1\n",
            "GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6pZ5hfvkrtMP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from collections import Counter\n",
        "from IPython.display import clear_output\n",
        "from pathlib import Path\n",
        "from random import random, randint,sample\n",
        "from time import time, sleep\n",
        "\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oqYnbmiRrtMP"
      },
      "outputs": [],
      "source": [
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware \"\n",
        "              \"accelerator.\")\n",
        "    if \"kaggle_secrets\" in sys.modules:\n",
        "        print(\"Go to Settings > Accelerator and select GPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDnv_Sx0rtMP"
      },
      "source": [
        "## Usando RNNs: Una estructura Encoder-Decoder para traducir de inglés a español"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBcIBBUArtMP"
      },
      "source": [
        "El primer gran avance por encima de la vectorización y las técnicas iniciales de resumen (summarization), traducción (translation), preguntas y respuestas (Q&A), es el empleo de redes recurrentes en el tratamiento de textos. Veamos un ejemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiDAs2-VrtMP"
      },
      "source": [
        "El objetivo del siguiente ejemplo es doble:  \n",
        "1. Mostrar cómo las redes recurrentes pueden configurarse para tratar un problema de traducción donde a una secuencia de entrada de longitud variable le corresponderá una secuencia de longitud variable y además no necesariamente coincidente, en dicha longitud, con la de entrada. Este problema se aplica a la traducción de inglés a español pero es un esquema que se puede emplear en cualquier tipo de cambio de representación entre secuencias (yes, para pasar de una secuencia a una imagen y viceversa).   \n",
        "\n",
        "2. Introducir de forma progresiva el concepto de _Atention_ (atención) para mejorar el modelo anterior y sobre este la arquitectura conocida como Transformers, que nos permitirá hablar de GPT, BERT y los LLM (Large Language Models, no Master of Laws, ojo) en general."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ81WAyHrtMQ"
      },
      "source": [
        "### El dataset de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh3YfbK_rtMQ"
      },
      "source": [
        "Utilizaremos un datset que empareja palabras y frases en inglés con sus traducciones al español de Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Kry1EyVWrtMQ"
      },
      "outputs": [],
      "source": [
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "path = tf.keras.utils.get_file(\"spa-eng.zip\",\n",
        "                               origin=url,\n",
        "                               cache_dir=\"datasets\",\n",
        "                               extract=True)\n",
        "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text(encoding = \"utf-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emipPlQartMQ"
      },
      "source": [
        "Observamos su contenido (siempre hay que mirar la mercancía antes de ponerse a cocinar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PsOOYjNrtMQ",
        "outputId": "282d40fe-21a9-4189-dcb7-166bf355d9de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Go.\tVe.\n",
            "Go.\tVete.\n",
            "Go.\tVaya.\n",
            "Go.\tVáyase.\n",
            "Hi.\tHola.\n",
            "Run!\t¡Corre!\n",
            "Run.\tCorred.\n",
            "Who?\t¿Quién?\n",
            "Fire!\t¡Fueg\n"
          ]
        }
      ],
      "source": [
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34GsOC45rtMQ"
      },
      "source": [
        "Lo transformamos para que sea una relación de secuencia a secuencia (dada una secuencia tenemos su target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8hoHlQwrtMR"
      },
      "outputs": [],
      "source": [
        "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
        "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
        "np.random.seed(42)  # extra code – ensures reproducibility on CPU\n",
        "np.random.shuffle(pairs)\n",
        "sentences_en, sentences_es = zip(*pairs)  # separa las parejas en dos listas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('How boring!',\n",
              " 'I love sports.',\n",
              " 'Would you like to swap jobs?',\n",
              " 'My mother did nothing but weep.',\n",
              " 'Croatia is in the southeastern part of Europe.',\n",
              " 'I have never eaten a mango before.',\n",
              " 'Tell the taxi driver to drive faster.',\n",
              " 'Tom and I work together.',\n",
              " 'I would prefer an honorable death.',\n",
              " 'Tom married a much younger woman.',\n",
              " \"It couldn't happen to me.\",\n",
              " \"Tom isn't going to marry you.\",\n",
              " \"Can you believe it? He's even lazier than me.\",\n",
              " 'She has hundreds of books.',\n",
              " 'They found him guilty.',\n",
              " 'Tom calls Mary up every night.',\n",
              " \"Tom says he won't come.\",\n",
              " \"I don't mind if the weather is hot.\",\n",
              " 'It is difficult to speak Chinese well.',\n",
              " \"Tom claimed that he didn't know Mary.\",\n",
              " 'Just come up here.',\n",
              " 'Tom has a strange way of talking.',\n",
              " 'Tom has many friends living in Boston.',\n",
              " 'I made my decision.',\n",
              " 'Come again tomorrow.',\n",
              " \"I think I'm still drunk from last night.\",\n",
              " 'My father and my brother work in this factory.',\n",
              " 'We need your signature.',\n",
              " \"Learn from others' mistakes.\",\n",
              " 'The dog walked backward.',\n",
              " \"I don't want to drink anything.\",\n",
              " 'Everybody likes him.',\n",
              " 'My father disapproved of my going to the concert.',\n",
              " 'She is a singer.',\n",
              " 'Tom has something in his right hand.',\n",
              " 'She is a leader in her field.',\n",
              " \"They're sunbathing around the pool.\",\n",
              " \"I think Tom didn't understand the joke.\",\n",
              " \"I don't like doctors.\",\n",
              " \"Don't laugh at him.\",\n",
              " 'The enemy attacked from behind.',\n",
              " 'Is Tom famous?',\n",
              " 'I asked him if I could read his book.',\n",
              " 'I have a slight pain in my side.',\n",
              " \"You don't speak English, do you?\",\n",
              " 'They are more or less the same size.',\n",
              " 'Neither of these is mine.',\n",
              " 'Tom is the person to ask.',\n",
              " \"I don't like this shirt. Show me another one.\",\n",
              " 'Tom wanted to know who Mary had been dating.',\n",
              " \"I didn't think this was your seat.\",\n",
              " 'The glass is dirty.',\n",
              " \"You didn't want to tell Tom about that, did you?\",\n",
              " \"You've misunderstood.\",\n",
              " 'Tom cheated.',\n",
              " 'The doctor took my pulse.',\n",
              " 'Tips are not accepted.',\n",
              " \"It's your turn to read.\",\n",
              " \"I wasn't careful.\",\n",
              " 'Enjoy your meal.',\n",
              " \"I don't mind sharing the room with him.\",\n",
              " \"You know me well enough to know I wouldn't do that.\",\n",
              " 'You should try the exam again.',\n",
              " 'He shot at the bird, but missed it.',\n",
              " \"I don't want trouble.\",\n",
              " 'Tom has no memory of what happened.',\n",
              " \"I don't know why I bother coming here.\",\n",
              " 'Is this a bad time to chat?',\n",
              " 'There were no radios in those days.',\n",
              " 'Does Tom have a girlfriend?',\n",
              " \"The defendant's innocence could not be verified.\",\n",
              " 'Who told you to bring me here?',\n",
              " 'What was there?',\n",
              " \"You told me it wouldn't rain today, so I didn't bring an umbrella.\",\n",
              " \"I don't like to be disturbed.\",\n",
              " \"Tom doesn't trust us.\",\n",
              " 'Please forgive me.',\n",
              " 'They will survey the desert island.',\n",
              " 'Can you speak a little slower?',\n",
              " \"Don't underestimate the problem.\",\n",
              " 'Tom took my car.',\n",
              " \"We didn't do anything wrong.\",\n",
              " 'It makes no difference to me whether he comes or not.',\n",
              " 'I love weddings.',\n",
              " 'I got jealous.',\n",
              " \"I think it's time for me to discuss the problem with her.\",\n",
              " 'Tom comes here every day.',\n",
              " 'Just ignore her.',\n",
              " \"There are few men who don't know that.\",\n",
              " 'Let me pay my share.',\n",
              " \"I've come back for you.\",\n",
              " 'Those books are mine.',\n",
              " 'The restaurant is next door to the theater.',\n",
              " 'I know what that means.',\n",
              " 'She is good at speaking English.',\n",
              " \"He's always joking.\",\n",
              " 'What did you do with that camera?',\n",
              " 'Just make this stop.',\n",
              " 'I saw many familiar faces.',\n",
              " 'If you are going to go to America, you should brush up your English.',\n",
              " 'Tom lives in the room above us.',\n",
              " 'Are you suggesting we run and hide?',\n",
              " 'She avoided answering my questions.',\n",
              " 'There is a great demand for gasoline.',\n",
              " 'My sister usually goes to the park every weekend.',\n",
              " 'Always keep a handkerchief in your pocket.',\n",
              " \"Please don't look over here again.\",\n",
              " \"I'm intrigued.\",\n",
              " 'I want to leave early.',\n",
              " 'The flowers in the vase were wilted.',\n",
              " \"I don't like talking about football.\",\n",
              " 'I could kill you.',\n",
              " 'This is smaller than that.',\n",
              " 'Mary has just come home.',\n",
              " 'I was concerned.',\n",
              " 'I found the room empty.',\n",
              " 'Tom bought a piece of land not far from where Mary lives.',\n",
              " 'Is this your dog?',\n",
              " \"Why don't you just shut up?\",\n",
              " 'Tom felt sick.',\n",
              " 'Are they satisfied?',\n",
              " 'Do you know anyone who hums while they work?',\n",
              " 'You lucky devil!',\n",
              " \"He got the twelve o'clock train.\",\n",
              " \"Tom isn't a crook.\",\n",
              " 'Nobody was tortured.',\n",
              " 'Tom wanted to help Mary.',\n",
              " 'It looks like Tom has an alibi for the night Mary was murdered.',\n",
              " 'I bought a hat at the store.',\n",
              " 'My sister is always weighing herself.',\n",
              " 'He seems to be asleep.',\n",
              " 'What time did you get here this morning?',\n",
              " \"I'm pretty hungry since I haven't eaten since early this morning.\",\n",
              " 'Tom is a former paratrooper.',\n",
              " 'We understand this.',\n",
              " 'He was pleased with the toy.',\n",
              " 'Tom has a reservation at this hotel.',\n",
              " \"Just don't tell anyone else.\",\n",
              " 'Tom followed us.',\n",
              " 'You are such a liar!',\n",
              " 'Are you single?',\n",
              " \"Don't worry about it. Everything's going to be fine.\",\n",
              " 'I called him, but a girl answered the phone.',\n",
              " 'She was asked to convince him to get his son to paint the house.',\n",
              " 'I like to relax with a good novel.',\n",
              " 'You should get your car fixed.',\n",
              " 'He is still here.',\n",
              " \"You're very astute.\",\n",
              " 'She hurt her elbow when she fell down.',\n",
              " 'Hold the ball with both hands.',\n",
              " 'Do you like Renaissance art?',\n",
              " 'Stop that car.',\n",
              " \"From this point, we'll go on foot.\",\n",
              " 'Tom had a very good time.',\n",
              " \"I told you I don't know if Tom will tell the truth.\",\n",
              " 'She fell asleep with her sweater on.',\n",
              " 'Tom stood up and headed for the bathroom.',\n",
              " 'Do you like them?',\n",
              " 'I noticed that she was wearing new glasses.',\n",
              " 'I never have had occasion to use it.',\n",
              " \"I've lost my ticket.\",\n",
              " 'Who else can answer my question?',\n",
              " \"Don't bother me with such trifles.\",\n",
              " 'I emailed Tom the pictures I took yesterday.',\n",
              " 'He made no response.',\n",
              " 'I hate the desert.',\n",
              " 'You have until midnight.',\n",
              " 'One of my friends says he wants me to meet you.',\n",
              " 'Tom told his son the story about a monster that ate children.',\n",
              " \"Don't eat too much.\",\n",
              " 'She has been watching television for three hours.',\n",
              " 'Can I try on this jacket?',\n",
              " \"I don't care.\",\n",
              " 'Tom changed his mind at the last minute.',\n",
              " \"Don't leave the bedroom window open.\",\n",
              " 'Try some.',\n",
              " 'Is anybody hurt?',\n",
              " 'His name escapes me.',\n",
              " 'She was busy with her knitting.',\n",
              " \"I wish the subway wasn't so crowded every morning.\",\n",
              " \"Maybe there's something I can do.\",\n",
              " \"I can't believe you married Tom.\",\n",
              " 'I see a lion.',\n",
              " \"This woman definitely knows that she doesn't know what she wants.\",\n",
              " 'We have less than two hours.',\n",
              " 'The bus stopped, but nobody got off.',\n",
              " 'Tom is a friend of mine.',\n",
              " 'She told me she knew my brother.',\n",
              " 'Almost no one believed her.',\n",
              " 'Will there be any food at the party?',\n",
              " 'You need to be more careful.',\n",
              " 'She talks a lot.',\n",
              " \"Why didn't Tom tell Mary?\",\n",
              " 'You survived.',\n",
              " \"They're not mine.\",\n",
              " 'Thank you for your thorough explanation.',\n",
              " 'I do not have much time.',\n",
              " 'She deserved it.',\n",
              " \"We're a bit busy at the moment. Can you hang on a minute?\",\n",
              " 'I wake him at six every morning.',\n",
              " \"I'll go provided you go with me.\",\n",
              " \"Speak louder. Your grandfather's hearing isn't so good.\",\n",
              " \"We don't often eat out.\",\n",
              " 'He licked his fingers.',\n",
              " 'I think the eggs that I just ate were rotten.',\n",
              " 'The tourists were ripped off at the nightclub.',\n",
              " 'She likes to listen to music.',\n",
              " 'How do you go to school every day?',\n",
              " \"Now that you're my girlfriend, I'm happy.\",\n",
              " \"Don't give me that look.\",\n",
              " 'I have no one to help me.',\n",
              " 'Tom thought it was strange.',\n",
              " \"Let's meet for a chat.\",\n",
              " 'There are many foreign tourists in Asakusa.',\n",
              " 'Raise your hands.',\n",
              " 'Stop right here.',\n",
              " 'Communism was the biggest issue in the campaign.',\n",
              " 'I go to school at eight in the morning.',\n",
              " 'Tom almost laughed.',\n",
              " 'These shoes are old, but I still like them.',\n",
              " 'What time do you want to meet?',\n",
              " 'I want to be a better person.',\n",
              " 'I brought you some food.',\n",
              " 'Tom was the last one to leave the party.',\n",
              " 'What time will you be back?',\n",
              " \"I really don't like goat cheese at all.\",\n",
              " 'He refused to sign the documents.',\n",
              " 'Long hair is out of fashion.',\n",
              " 'I like tennis.',\n",
              " \"It's true that he is in love with her.\",\n",
              " 'I read the book from beginning to end.',\n",
              " \"We're studying physical science.\",\n",
              " 'My grandmother lived to be ninety-five years old.',\n",
              " 'She disappeared in the dark.',\n",
              " 'Tom kissed Mary on her forehead.',\n",
              " 'I have plenty of things to eat in the pantry.',\n",
              " 'He was kind enough to help me with my homework.',\n",
              " 'All you do is complain!',\n",
              " \"There's nowhere to hide.\",\n",
              " \"Don't phone me when I'm at the office.\",\n",
              " 'Let me help.',\n",
              " 'Do you know how to use a computer?',\n",
              " 'The pond dried up last summer.',\n",
              " \"We're married to each other.\",\n",
              " \"That's not how you spell my name.\",\n",
              " \"That's perfect.\",\n",
              " 'Ask him for advice.',\n",
              " 'He went to bed at eleven last night.',\n",
              " \"I'm very modest.\",\n",
              " \"I'm afraid of dogs.\",\n",
              " 'This is the place where Tom was born.',\n",
              " 'Tom lived in a small fishing village.',\n",
              " \"I thought I'd always be alone.\",\n",
              " \"What's the airmail rate?\",\n",
              " 'Many a mother spoils her sons by not being strict enough.',\n",
              " 'He got off the bus.',\n",
              " \"The medicine she took cured her of the bad cough she'd been suffering from.\",\n",
              " 'You will get well in a week or so.',\n",
              " 'I am dying to see her again.',\n",
              " 'Tom managed a small bar near Boston for quite a few years.',\n",
              " 'Is one thousand yen enough?',\n",
              " 'His old cat is still alive.',\n",
              " \"We don't have time.\",\n",
              " \"I'll be there.\",\n",
              " 'Did you come to town?',\n",
              " 'Why is this happening?',\n",
              " 'Tom plugged in his computer.',\n",
              " 'Nobody told me what time I should come.',\n",
              " \"I'm crazy.\",\n",
              " \"It shouldn't happen again.\",\n",
              " \"They're right behind you.\",\n",
              " 'She took a deep breath.',\n",
              " 'Do we have enough silverware for thirty people?',\n",
              " 'Tom is hyperventilating.',\n",
              " 'Everyone looks worried.',\n",
              " 'These glasses are cool.',\n",
              " 'There was no other choice but to abandon the entire project.',\n",
              " \"It's an interesting argument.\",\n",
              " \"That's cruel.\",\n",
              " 'I watch television in the evening.',\n",
              " 'Do you play the piano?',\n",
              " \"I'm here to save you.\",\n",
              " \"We're not eating.\",\n",
              " 'Send this message to as many people as you can.',\n",
              " 'She drew out the money from the bank.',\n",
              " \"Don't let anyone leave this building.\",\n",
              " 'Tom talked Mary into inviting John to the party.',\n",
              " 'I see a star.',\n",
              " 'He finally fulfilled my request.',\n",
              " 'I visited many parts of England.',\n",
              " 'He deserves more.',\n",
              " 'I apologized.',\n",
              " 'Those are not your chairs.',\n",
              " \"He's a DJ.\",\n",
              " 'I am divorced.',\n",
              " 'Keep an eye on Tom.',\n",
              " 'How many pens are there on the desk?',\n",
              " \"They didn't seem to notice it.\",\n",
              " 'My daugther wants a kitten.',\n",
              " 'Pay your fare here.',\n",
              " \"Don't bite the hand that feeds you.\",\n",
              " 'My roommate is too talkative.',\n",
              " 'Everyone dies eventually.',\n",
              " \"What's wrong with you?\",\n",
              " 'I want you to help us find out who killed Tom.',\n",
              " 'Tom likes animals.',\n",
              " 'We should obey the traffic rules.',\n",
              " \"Fiction is obliged to stick to possibilities. Truth isn't.\",\n",
              " 'I feel helpless.',\n",
              " 'I have a pain in my foot.',\n",
              " \"A gun won't do you much good if you're not willing to shoot it.\",\n",
              " 'I am going to go to America next year.',\n",
              " 'Will this be painful?',\n",
              " 'Where should we meet?',\n",
              " \"Tom's youngest daughter is his favorite.\",\n",
              " 'If I had time, I would study French.',\n",
              " 'When can I visit you?',\n",
              " 'He is still angry.',\n",
              " 'Give me a second.',\n",
              " 'How many pencils do you have?',\n",
              " \"I've done it before.\",\n",
              " 'Tom and Mary went deer hunting.',\n",
              " 'I will start tonight.',\n",
              " 'Students may not enter the faculty lounge.',\n",
              " \"Tom didn't pull the trigger.\",\n",
              " 'What happened to you? You look miserable.',\n",
              " 'She drinks like a fish.',\n",
              " 'Shall I prepare you a warm meal?',\n",
              " 'My father bought me a camera for my birthday.',\n",
              " \"His advice didn't help much.\",\n",
              " 'What are you doing on your night off?',\n",
              " 'Whoever wins the race will receive the prize.',\n",
              " \"You're the most handsome man I've ever seen.\",\n",
              " 'My passport was stolen.',\n",
              " 'That guy is totally nuts!',\n",
              " 'I just arrived now.',\n",
              " 'I hate my boss.',\n",
              " 'It was my first night among strangers.',\n",
              " 'Tom agreed.',\n",
              " 'He went away without saying a word.',\n",
              " 'I guess you will be very busy tonight.',\n",
              " 'He is afraid of making mistakes.',\n",
              " 'Tom wondered what Mary was doing here.',\n",
              " \"We'll hide it.\",\n",
              " \"He apologized for his rudeness, but she wouldn't forgive him.\",\n",
              " 'A cargo vessel, bound for Athens, sank in the Mediterranean without a trace.',\n",
              " 'When the man saw a policeman, he fled.',\n",
              " 'We have other things to do.',\n",
              " 'Where did you hide it?',\n",
              " 'I thanked Mary for her help.',\n",
              " 'You might get injured.',\n",
              " 'Terrorists blew up a bus.',\n",
              " 'He could not speak French well.',\n",
              " 'He stopped playing baseball last season.',\n",
              " 'We have to use the stairs, because the elevator is being repaired.',\n",
              " 'Our school did away with uniforms last year.',\n",
              " 'This antique clock is worth one thousand dollars.',\n",
              " 'That really bothers me.',\n",
              " 'We hardly ever see you around here anymore.',\n",
              " 'She belongs to the Democratic Party.',\n",
              " 'I looked around.',\n",
              " \"Let's make it brief.\",\n",
              " 'Do you have a house?',\n",
              " 'Doctors use medical equipment.',\n",
              " \"These are Tom's ski boots.\",\n",
              " \"I'd appreciate it if you would turn out the lights.\",\n",
              " 'Put the broom in the closet.',\n",
              " 'What do you want me to do?',\n",
              " 'I would like to offer you the position.',\n",
              " \"I can't put up with his violence any longer.\",\n",
              " \"It's only the beginning.\",\n",
              " 'I really got depressed.',\n",
              " 'I use it every day.',\n",
              " 'She is incapable of deceit.',\n",
              " 'I threw away my shoes.',\n",
              " \"I didn't get you a present.\",\n",
              " 'I was overconfident.',\n",
              " \"Tom is very dependable, isn't he?\",\n",
              " 'The sun is shining brightly.',\n",
              " 'He does not have any friends.',\n",
              " \"I'll eat here.\",\n",
              " 'Keep on smiling.',\n",
              " 'Someone is standing behind the bushes taking pictures of us.',\n",
              " 'Playing tennis is fun.',\n",
              " 'Tom never seems to get upset.',\n",
              " 'The furniture was dusty.',\n",
              " \"I don't know anybody in this town.\",\n",
              " 'I bought her a watch.',\n",
              " \"Let's go swimming.\",\n",
              " 'OK, listen up.',\n",
              " 'The teacher intervened in the quarrel between the two students.',\n",
              " 'That was a close call.',\n",
              " 'Everyone remarked on his new hairstyle.',\n",
              " \"He doesn't have a job. He's retired.\",\n",
              " 'I like him a lot, but sometimes he gets on my nerves.',\n",
              " 'Old people are usually very wise.',\n",
              " \"Don't you think it a bad thing?\",\n",
              " 'I feel like doing something different today.',\n",
              " 'I had no trouble finding his office.',\n",
              " 'Tom kept trying to call Mary, but she never answered her phone.',\n",
              " 'The patient fainted at the sight of blood.',\n",
              " 'This is how we cook rice.',\n",
              " \"I'm selling my car at a loss.\",\n",
              " 'Take a few days off.',\n",
              " 'I love French movies.',\n",
              " 'Tom raised his hand.',\n",
              " 'Tom congratulated Mary for her driving test.',\n",
              " 'Ask him when the next plane leaves.',\n",
              " 'What time does the club open?',\n",
              " 'The judge sentenced him to one year in prison.',\n",
              " \"He's wearing a white cotton shirt.\",\n",
              " 'He was very worried about having to spend Christmas in the hospital.',\n",
              " 'I thought you were going to kill me.',\n",
              " 'Cookie is my dog.',\n",
              " 'Is this correct?',\n",
              " 'I just want you to be involved.',\n",
              " 'What was the result?',\n",
              " \"Don't think I didn't try.\",\n",
              " 'I hate it.',\n",
              " 'I will never make that mistake again.',\n",
              " 'My son gets on very well at school.',\n",
              " 'How well can you skate?',\n",
              " \"He's not all there.\",\n",
              " 'My dream is to be a firefighter.',\n",
              " 'The storm let up.',\n",
              " 'Get to bed.',\n",
              " 'I already told you it was an accident.',\n",
              " \"Tom's car is in the garage.\",\n",
              " 'He died two hours later.',\n",
              " \"I'll tell you what happened.\",\n",
              " 'The box is heavy.',\n",
              " 'Please come and see me if you have time.',\n",
              " 'They bought a few pieces of furniture when they got married.',\n",
              " \"I think I'm in trouble.\",\n",
              " 'Tom felt uneasy talking to Mary about that matter.',\n",
              " 'That stove smokes too much.',\n",
              " \"I didn't know how to respond.\",\n",
              " 'He died of old age two years ago.',\n",
              " 'His car has just been repaired.',\n",
              " \"If you don't study harder, you'll definitely fail.\",\n",
              " 'He asked me to read 5 poems.',\n",
              " 'What have you told them?',\n",
              " \"What if I'm right?\",\n",
              " 'Tom was convicted and sentenced to death.',\n",
              " \"I don't want to do this anymore.\",\n",
              " 'I do the laundry on Sundays.',\n",
              " \"It's best to wear a cap on your head during the cold Moscow winters.\",\n",
              " 'I could feel nothing but the knife as it plunged into my back.',\n",
              " \"I'm sorry, but I can't go with you.\",\n",
              " 'How rude of you!',\n",
              " 'He has a nice income.',\n",
              " 'That shirt looks good on you.',\n",
              " 'You did your best.',\n",
              " \"Are you absolutely sure you want to sell your father's guitar?\",\n",
              " 'I jog twice a week.',\n",
              " \"I'm fed up with your constant complaining.\",\n",
              " 'It would be best if I met him in person.',\n",
              " 'That is the shop where I used to work.',\n",
              " \"Why didn't you try the dress on before you bought it?\",\n",
              " 'It was raining heavily in Osaka.',\n",
              " 'What does she look like?',\n",
              " \"I haven't made up my mind.\",\n",
              " 'That we are not able to do.',\n",
              " 'My dream is to become a famous singer.',\n",
              " 'Please lend me this book for a few days.',\n",
              " 'Prices rose drastically as a result of this policy.',\n",
              " 'Where did you buy this guitar?',\n",
              " \"Don't let go.\",\n",
              " \"I didn't mean to hit him.\",\n",
              " 'Did Tom tell you anything interesting?',\n",
              " 'Customers stopped coming to our shop.',\n",
              " 'Do you still love Tom?',\n",
              " 'He expressed himself clearly.',\n",
              " 'Tom noticed that Mary was limping.',\n",
              " 'Just come up here.',\n",
              " \"Tom can't take less.\",\n",
              " 'Use it or lose it.',\n",
              " 'They sat by the fireplace.',\n",
              " \"Tom didn't know how to treat his employees right.\",\n",
              " 'Tom told Mary the bad news.',\n",
              " 'I am a student in a university.',\n",
              " 'I want to know who killed Tom.',\n",
              " 'The machine operates around the clock.',\n",
              " 'What a good scholar the author must be to write such a splendid book!',\n",
              " 'The policeman separated the two men who were fighting.',\n",
              " 'Why are you so sure Tom is Canadian?',\n",
              " 'A century is one hundred years.',\n",
              " 'Tom lay awake for a long time thinking about Mary.',\n",
              " \"I'm impartial.\",\n",
              " 'The frescoes of the cathedral are very interesting.',\n",
              " 'My book is here.',\n",
              " \"There's a soccer match tomorrow.\",\n",
              " 'The influence of TV on society is great.',\n",
              " 'Do you want to go to the zoo?',\n",
              " 'Tom first met Mary when they were in high school.',\n",
              " 'My brother is a vet.',\n",
              " 'I think Tom is going to die.',\n",
              " \"I don't believe Tom can do that.\",\n",
              " 'What did Tom tell Mary not to do?',\n",
              " 'The mall is deserted.',\n",
              " 'What do you believe?',\n",
              " 'She said she was ill in bed, which was a lie.',\n",
              " 'I want to be rich.',\n",
              " 'What train you are going to take?',\n",
              " 'The floor was very cold.',\n",
              " \"Please don't use English.\",\n",
              " 'Tom is very smart, just like you.',\n",
              " 'I have no intention of getting wet.',\n",
              " \"The man's behavior was very odd.\",\n",
              " \"Tom and I could've saved ourselves a lot of time by just doing that ourselves.\",\n",
              " 'There was a striking resemblance between them.',\n",
              " \"That man's not to be trusted.\",\n",
              " 'Besides being a doctor, he was a very famous novelist.',\n",
              " \"To tell the truth, I didn't do my homework.\",\n",
              " 'They sat down.',\n",
              " 'He had a traffic accident on his way to school.',\n",
              " 'The desire to fly in the sky like a bird inspired the invention of the airplane.',\n",
              " 'That typhoon prevented me from going out.',\n",
              " \"I couldn't resist the urge to applaud.\",\n",
              " 'Are you done with your homework yet?',\n",
              " \"I'm getting undressed.\",\n",
              " 'The story is full of holes.',\n",
              " 'I gave my books to those people.',\n",
              " \"We're even.\",\n",
              " \"He's leaving for China tomorrow.\",\n",
              " 'I often watch TV before dinner.',\n",
              " 'Try us again next Monday.',\n",
              " 'She was depressed by all her problems.',\n",
              " 'The boy is afraid of the dark.',\n",
              " 'My mother loves me.',\n",
              " 'Tom disliked school when he was younger.',\n",
              " 'Will you let me use your telephone, please?',\n",
              " 'Give me your sandwich.',\n",
              " 'He promised not to smoke.',\n",
              " 'Those are the risks.',\n",
              " 'No one voted against it.',\n",
              " 'My life was in danger.',\n",
              " 'The police have caught him.',\n",
              " 'She taught me how to make a web site.',\n",
              " 'The more a man knows, the more he discovers his ignorance.',\n",
              " 'There are four people in my family.',\n",
              " 'Babies cry when they are hungry.',\n",
              " \"It's incredible!\",\n",
              " 'John met Mary on his way to school.',\n",
              " \"The child slept on its mother's lap.\",\n",
              " \"I don't like visiting big cities.\",\n",
              " 'This is the umbrella I bought yesterday.',\n",
              " 'I really like the concept of this website.',\n",
              " 'Go back to work.',\n",
              " 'Tom was the sort of man you could get along with.',\n",
              " 'Which one are you going to use?',\n",
              " \"Do you think I'm stupid?\",\n",
              " 'Tom showed me a magic trick.',\n",
              " 'She was wearing an ugly dress.',\n",
              " 'His ideas are too radical to be acceptable to most people.',\n",
              " 'You should cut down on the amount of fattening food that you eat.',\n",
              " 'It is clear that he is guilty.',\n",
              " 'You need to shut up.',\n",
              " 'She arrived just as I was leaving.',\n",
              " 'She does not have much money.',\n",
              " \"We're doing fine.\",\n",
              " \"The movie wasn't as interesting as the book.\",\n",
              " \"This clock isn't working.\",\n",
              " 'I ate a hot dog for lunch.',\n",
              " \"Don't forget about me.\",\n",
              " 'The house is on fire.',\n",
              " \"I don't understand French at all.\",\n",
              " 'The gasoline truck ran into the gate and blew up.',\n",
              " 'He has to work hard in order to support his family.',\n",
              " 'Will you tell me the truth?',\n",
              " 'Keep warm.',\n",
              " 'His grandfather is what is called a self-made man.',\n",
              " 'I got carded.',\n",
              " 'He confessed all his sins.',\n",
              " \"I'll go change my clothes.\",\n",
              " 'Could you please take me back home?',\n",
              " \"Don't forget to call me.\",\n",
              " 'The news made him happy.',\n",
              " 'The animals had to be killed.',\n",
              " 'Illness forced him to give up school.',\n",
              " 'He regretted not having taken my advice.',\n",
              " 'I think Tom drank out of my glass by mistake.',\n",
              " 'Why are you angry with me?',\n",
              " 'Let the children play.',\n",
              " 'He bored us with his long stories.',\n",
              " \"I don't know about you, but I'm starved.\",\n",
              " 'I have a lot of things to tell you.',\n",
              " 'Boys are more aggressive than girls.',\n",
              " 'He always borrows money from me.',\n",
              " 'He wants to study music and dance.',\n",
              " 'They left.',\n",
              " 'Are you a ghost?',\n",
              " \"Don't leave the windows open.\",\n",
              " 'Cut it out, Tom.',\n",
              " \"I'm eating now.\",\n",
              " 'Tom stands by me whenever I am in trouble.',\n",
              " 'He earns over 500 dollars a month from that job.',\n",
              " 'I told you to be careful.',\n",
              " 'They never tell a lie.',\n",
              " 'Have you ever seen anything so beautiful?',\n",
              " 'What a shame!',\n",
              " 'We need to chat soon.',\n",
              " 'She introduced her sister to him more than two years ago.',\n",
              " \"Didn't you hear the scream?\",\n",
              " 'Tom was lost.',\n",
              " 'Are you 18?',\n",
              " 'I am a member of the basketball team.',\n",
              " 'Tom is a diabetic.',\n",
              " 'I got out of the car at 40th Street.',\n",
              " 'Sorry for the delay.',\n",
              " 'Is English spoken in Canada?',\n",
              " \"Tom thought Mary wouldn't want to live in Boston.\",\n",
              " 'Tom told me I should get myself a girlfriend.',\n",
              " \"You didn't invite me.\",\n",
              " 'Does it hurt when you chew?',\n",
              " \"You can run, but you can't hide.\",\n",
              " 'She acted in the play.',\n",
              " 'I just want to hug you.',\n",
              " 'Tom leaned on his cane.',\n",
              " 'That makes no sense at all.',\n",
              " 'I want to buy this jacket.',\n",
              " 'He deposited 100 dollars in his saving account.',\n",
              " \"I'm not as healthy as I used to be.\",\n",
              " 'Yes, I know it.',\n",
              " 'I hope that you will get well soon.',\n",
              " 'Her house is close to the park.',\n",
              " 'About how many books do you have?',\n",
              " 'I was at home almost all day yesterday.',\n",
              " 'He is blinded by love.',\n",
              " 'He had few friends and little money.',\n",
              " 'He is a scientist who is respected by everybody.',\n",
              " 'They say that she is in love with him.',\n",
              " 'Tell her to not look for me.',\n",
              " \"When a good opportunity presents itself, I don't let it pass.\",\n",
              " 'She has a cold and is absent from school.',\n",
              " 'The library is on the 4th floor.',\n",
              " \"I haven't yet read all of these books.\",\n",
              " \"Mary doesn't have expensive tastes.\",\n",
              " 'He has a lot of original ideas.',\n",
              " 'I explained the rules to her.',\n",
              " 'We were to be married in May but had to postpone the marriage until June.',\n",
              " 'The computer is to her left.',\n",
              " 'He has a lot of money.',\n",
              " \"I'll be back in two hours.\",\n",
              " 'It is better to live rich, than to die rich.',\n",
              " \"There's a long line at every cash register.\",\n",
              " \"I'm sure you're going to like this lunch.\",\n",
              " 'We jog before breakfast every morning.',\n",
              " 'Nobody laughed.',\n",
              " 'I have already finished reading this book.',\n",
              " \"We haven't seen it yet.\",\n",
              " \"What's in that cupboard?\",\n",
              " 'Good for you.',\n",
              " \"Let's play something.\",\n",
              " 'Next Monday is a holiday.',\n",
              " 'This is the only book I have.',\n",
              " 'Is he aware of the difficulty?',\n",
              " 'I wanted us to be happy.',\n",
              " 'I believe in fate.',\n",
              " 'Tom saw his reflection in the mirror.',\n",
              " 'Everybody needs one.',\n",
              " 'Tom had no further questions.',\n",
              " 'Will you please show me the way?',\n",
              " 'I know a shortcut.',\n",
              " 'What are those?',\n",
              " \"We haven't got much time.\",\n",
              " \"It's a lot of fun to be with you.\",\n",
              " 'Could you just get straight to the point?',\n",
              " \"That's the right answer.\",\n",
              " 'Would you like to drink anything?',\n",
              " 'Is this love?',\n",
              " \"She starts her job at seven o'clock.\",\n",
              " \"Let's go to a movie.\",\n",
              " \"He hasn't made a record or had a concert for many years.\",\n",
              " 'You look tired.',\n",
              " \"I'm looking for my brother.\",\n",
              " 'The kitten slept soundly.',\n",
              " 'She married him.',\n",
              " 'He is the older of the two.',\n",
              " '\"She likes music.\" \"So do I.\"',\n",
              " 'If she were here now, I would tell her the truth.',\n",
              " 'My shopping bag broke.',\n",
              " 'We were friends.',\n",
              " 'Everyone cheered.',\n",
              " 'These tips may save your life.',\n",
              " 'Where are your friends?',\n",
              " \"Why shouldn't truth be stranger than fiction? Fiction, after all, has to make sense.\",\n",
              " 'Come on, give it to me.',\n",
              " 'Tom also plays the violin.',\n",
              " 'Tom admitted that he was afraid.',\n",
              " 'Tom is clearly worn out.',\n",
              " 'The dogs bayed at the full moon.',\n",
              " \"That's obscene.\",\n",
              " 'They went fishing yesterday.',\n",
              " 'He took out his handkerchief.',\n",
              " 'I was expecting you.',\n",
              " 'What do you have on the menu today?',\n",
              " 'Tom made it quite clear what he wanted.',\n",
              " 'This is really cool.',\n",
              " 'She needs our help.',\n",
              " 'She is out now.',\n",
              " 'They abandoned their children in the forest.',\n",
              " 'Tom got to the airport just in the nick of time.',\n",
              " \"Tom said he couldn't clean the pool tomorrow afternoon.\",\n",
              " 'Tom probably knows the answer.',\n",
              " \"It's understood that we'll start tomorrow.\",\n",
              " 'Hit Tom.',\n",
              " 'I handed in my report yesterday.',\n",
              " 'I have neglected you so long that I feel a bit shy in visiting you.',\n",
              " 'Give me a hammer.',\n",
              " 'I have a small fever.',\n",
              " \"It must've cost a fortune.\",\n",
              " 'I lied.',\n",
              " 'I could have sworn I saw somebody.',\n",
              " 'I suppose you like him.',\n",
              " \"I didn't know anyone at the party.\",\n",
              " \"You want a divorce, don't you?\",\n",
              " 'I am a teacher of English.',\n",
              " 'You must be more polite.',\n",
              " 'Who knows?',\n",
              " 'She asked me if I knew her address.',\n",
              " 'I once lived in Rome.',\n",
              " \"I'll scold him.\",\n",
              " 'Tom ate what little food he had left.',\n",
              " 'A policeman was gazing at a suspicious pedestrian.',\n",
              " 'I never looked for you.',\n",
              " 'Tom once worked at a bakery.',\n",
              " \"Everyone looked at Tom to see what he'd do.\",\n",
              " 'Tom was abused by his father.',\n",
              " \"Just a moment. I haven't made up my mind yet.\",\n",
              " 'It was the biggest mistake of my life.',\n",
              " 'Tom is the one who feeds the dog.',\n",
              " 'I will leave as soon as the bell rings.',\n",
              " 'Would you speak more slowly, please?',\n",
              " 'I am not a doctor, but a teacher.',\n",
              " 'Neither of those boys can speak French.',\n",
              " \"I'm learning English.\",\n",
              " 'Do you know of any good restaurants around here?',\n",
              " 'His brother had been missing for a while.',\n",
              " 'I looked down.',\n",
              " 'Please close the drapes.',\n",
              " 'His breath smells like goat cheese.',\n",
              " 'Nothing is happening.',\n",
              " \"He didn't have enough money to ride home on the train.\",\n",
              " 'I am delighted to be here.',\n",
              " \"Let's start with leftovers.\",\n",
              " 'What time does the show start?',\n",
              " 'The studio is very small, with no place to hide.',\n",
              " \"I don't mind sleeping on the floor.\",\n",
              " 'Tom never helps me.',\n",
              " 'Only one little boy survived the traffic accident.',\n",
              " \"Tom won't be here next month.\",\n",
              " 'You should go to the barbershop.',\n",
              " 'Our car ran out of gas after ten minutes.',\n",
              " 'Whose bicycle is this?',\n",
              " 'He never stopped writing.',\n",
              " 'She was restless because she did not have anything to do.',\n",
              " 'Put it in first and slowly let off the clutch while giving it a little gas.',\n",
              " 'How would you translate this sentence?',\n",
              " 'Do you eat in the classroom?',\n",
              " \"I wish I'd studied French when I was younger.\",\n",
              " 'Your questions were too direct.',\n",
              " 'Can you remember the first time we met each other?',\n",
              " 'I have plenty of time to do that.',\n",
              " 'I know Tom from work.',\n",
              " 'What have you done?',\n",
              " 'It is said the house is haunted.',\n",
              " 'Last year in the Philippines, earthquakes and tidal waves resulted in the deaths of more than 6,000 people.',\n",
              " \"I'm very grateful to you.\",\n",
              " \"Most people don't have a problem with that.\",\n",
              " \"I can't stand that noise.\",\n",
              " 'Everything will change.',\n",
              " 'None of us is perfect.',\n",
              " \"I'll go back to Boston.\",\n",
              " \"This museum isn't open on Mondays.\",\n",
              " 'Does he know how you feel?',\n",
              " \"They don't know that I'm Japanese.\",\n",
              " 'He often goes off on wild goose chases.',\n",
              " \"My father's hobby is fishing.\",\n",
              " 'I had stuff to do.',\n",
              " 'He was too drunk to remember to shut the back door.',\n",
              " 'I cut a branch from the tree.',\n",
              " 'How many days do we have left until summer vacation begins?',\n",
              " \"How's business?\",\n",
              " 'Close your mouth.',\n",
              " \"Tom doesn't want to do anything but swim.\",\n",
              " 'All men are equal.',\n",
              " 'I have an electric guitar.',\n",
              " 'Would you please stop singing so loudly? This is not a cheap drinking place.',\n",
              " 'Which do you like better, the Giants or the Dragons?',\n",
              " \"I don't feel like playing tennis today.\",\n",
              " 'He looks well.',\n",
              " 'Do you want to make some money today?',\n",
              " 'He bowed to the Queen.',\n",
              " \"She says you'll bring some friends along.\",\n",
              " \"Tom's funny.\",\n",
              " 'Tom is taller and stronger than Mary.',\n",
              " 'He abandoned his family.',\n",
              " 'Are there a lot of Moroccans in Germany?',\n",
              " 'Every mistake made me stronger.',\n",
              " 'Did she show you the picture?',\n",
              " 'Look at yourself in the mirror.',\n",
              " 'I wish I could speak French like a native speaker.',\n",
              " 'Tom asked Mary about her new job in Boston.',\n",
              " \"Tom doesn't have to wash the car. Mary's already washed it.\",\n",
              " 'It was yesterday.',\n",
              " \"What's your philosophy?\",\n",
              " \"What's the spiciest thing you've ever eaten?\",\n",
              " 'I love to teach.',\n",
              " \"Tom is a friend of Mary's.\",\n",
              " 'Who killed Tom?',\n",
              " \"You're soaking wet.\",\n",
              " 'You may park here.',\n",
              " 'The children amused themselves by playing games.',\n",
              " 'What are you smirking at?',\n",
              " 'They know what happened.',\n",
              " 'Where can I buy a map?',\n",
              " 'We go to church together.',\n",
              " 'I knew Tom would be there.',\n",
              " 'Tom and Mary know each other.',\n",
              " 'Will you pick out a tie for me?',\n",
              " \"I knew I shouldn't have put off doing my homework until the last minute.\",\n",
              " 'You are prohibited from smoking here.',\n",
              " 'What are your intentions?',\n",
              " 'I cannot excuse her.',\n",
              " 'Tom came back home after dark.',\n",
              " 'Tom is improving.',\n",
              " 'Life is like riding a bicycle. To keep your balance you must keep moving.',\n",
              " 'When the phone rang, he sprang out of bed.',\n",
              " 'Can you recommend a good game to me?',\n",
              " 'Tom has wavy brown hair and blue eyes.',\n",
              " 'Tom is useless.',\n",
              " 'When I met her the other day she asked of my parents.',\n",
              " \"Tom loved the color of Mary's new dress.\",\n",
              " 'You are responsible for the death of the child.',\n",
              " 'Where is your cap?',\n",
              " \"I'm severely allergic to peanuts.\",\n",
              " 'How many galaxies are there in the universe?',\n",
              " 'There was no choice but to sit and wait.',\n",
              " 'Tom asked Mary which way to turn.',\n",
              " 'Her composition was free from mistakes.',\n",
              " 'The store is closed until further notice.',\n",
              " \"I think I've broken my arm.\",\n",
              " 'He has a mild nature.',\n",
              " 'Go first to those who you are sure will help you.',\n",
              " \"Now what's wrong?\",\n",
              " 'The village is connected with our town by a bridge.',\n",
              " 'The milk tasted sour.',\n",
              " 'I confessed to stealing the money.',\n",
              " \"We're an hour behind.\",\n",
              " 'This CD belongs to her.',\n",
              " 'He was scared you would shoot him.',\n",
              " 'He has a dog.',\n",
              " 'She kept on talking while eating.',\n",
              " 'I came to talk to you.',\n",
              " 'It took less than five minutes.',\n",
              " 'Everybody needs to know this.',\n",
              " 'I agree with him on that point.',\n",
              " 'We got in after a long wait.',\n",
              " \"You've put on weight, haven't you?\",\n",
              " 'Could I park my car here?',\n",
              " 'How long is your Christmas vacation?',\n",
              " 'Tom had trouble resolving the situation.',\n",
              " 'The author of this book is still young.',\n",
              " 'I have the ace of clubs.',\n",
              " \"He doesn't want to go to school today.\",\n",
              " \"If you make new friends, don't forget the old ones.\",\n",
              " 'Try harder tomorrow.',\n",
              " 'Are you the owner of this house?',\n",
              " 'Help us.',\n",
              " 'I stopped smoking.',\n",
              " 'This song is known to everyone.',\n",
              " 'You look hot.',\n",
              " 'Tom put himself and his children at risk.',\n",
              " 'I wish I were clever.',\n",
              " 'I was in São Paulo in February.',\n",
              " \"I don't blame you for putting off our trip.\",\n",
              " 'Where do you think all the money goes?',\n",
              " \"I hope you're not mad at me.\",\n",
              " \"I haven't eaten there in a long time.\",\n",
              " 'Please take off your hat.',\n",
              " 'Tom twisted his ankle.',\n",
              " 'My uncle is very fond of fishing.',\n",
              " 'Who is the tallest of the five?',\n",
              " \"Tom and Mary didn't get along very well.\",\n",
              " 'I can resist everything but temptation.',\n",
              " 'Sanitary conditions in the refugee camps were terrible.',\n",
              " 'He was brave.',\n",
              " 'Tom is deceitful.',\n",
              " \"He's already left.\",\n",
              " \"Here's the key to your room.\",\n",
              " 'I need a miracle.',\n",
              " 'My glass is empty.',\n",
              " \"Tom and Mary don't have much time to talk together. Their children are always demanding their attention.\",\n",
              " 'It takes more time to prepare the car for getting painted than it takes for the actual paint job itself.',\n",
              " 'Do you need to leave today?',\n",
              " \"He's not home.\",\n",
              " \"You shouldn't always follow the crowd.\",\n",
              " 'Nobody lives here.',\n",
              " 'Be creative.',\n",
              " 'The boy tried to be a man and not cry.',\n",
              " 'Who does she think that she is?',\n",
              " \"Let's pool all our money.\",\n",
              " \"I'm not sure there's a problem.\",\n",
              " 'Tom already ate.',\n",
              " 'What an idiot!',\n",
              " 'Are you embarrassed?',\n",
              " 'I only have a few books.',\n",
              " 'Where do they do that?',\n",
              " 'It makes no difference to me whether he likes baseball or football.',\n",
              " \"Mom, where's the cat?\",\n",
              " 'When did you start studying Latin?',\n",
              " 'Tom has to look after Mary.',\n",
              " 'Tom stayed up all night studying.',\n",
              " 'Show me another watch.',\n",
              " 'Of course, many senior citizens are happy with retirement.',\n",
              " 'This dictionary, of which the third volume is missing, cost me a hundred dollars.',\n",
              " 'We estimated the damage at 1000 dollars.',\n",
              " 'Some did not know how to fight.',\n",
              " 'Health is essential to happiness.',\n",
              " 'I could not get out of the stadium because of the crowd.',\n",
              " \"Do you really think it's bad?\",\n",
              " 'How about going for a drive?',\n",
              " \"I didn't speak with Tom.\",\n",
              " 'He arrived in time.',\n",
              " 'Our sales are decreasing.',\n",
              " 'No way!',\n",
              " 'She came to see us yesterday.',\n",
              " 'Tom must be angry with Mary.',\n",
              " 'Tom is a good athlete.',\n",
              " 'The old house was demolished.',\n",
              " 'How big you are!',\n",
              " 'We have yellow apples.',\n",
              " 'The only time you talk to me is when you need some money.',\n",
              " 'We slept under the stars.',\n",
              " 'He said that that girl had kissed him.',\n",
              " 'There was a bus in the way.',\n",
              " 'Coming back was a bad choice.',\n",
              " \"How's everyone at the Hong Kong office?\",\n",
              " 'Tom sings quite well.',\n",
              " 'Absence makes the heart grow fonder.',\n",
              " \"Your soup's getting cold.\",\n",
              " \"You're driving me nuts.\",\n",
              " \"He's stronger than you.\",\n",
              " \"I can't untie this knot.\",\n",
              " \"I should've eaten more.\",\n",
              " 'Is he a doctor?',\n",
              " 'I read a letter.',\n",
              " 'You have no right to do this.',\n",
              " \"I can't make promises.\",\n",
              " \"It's been thirty years since we got married.\",\n",
              " 'The violence lasted for two weeks.',\n",
              " 'Women generally live longer than men.',\n",
              " 'I thought Tom would want to try some Japanese food.',\n",
              " \"I'm sure there is nobody as kind as you are the whole world.\",\n",
              " \"Don't laugh at him.\",\n",
              " 'You must remove your shoes before entering a house.',\n",
              " 'Can we move on?',\n",
              " 'She was soaked from head to foot.',\n",
              " \"I didn't look under the couch.\",\n",
              " 'When I was a boy, I thought that I wanted to be a doctor.',\n",
              " \"I'll be with you in a second.\",\n",
              " 'I agree with you that we need more women in this company.',\n",
              " 'How could that happen?',\n",
              " \"What's Germany's largest lake?\",\n",
              " 'My mother has sold everything that is dear to her.',\n",
              " 'Is that it?',\n",
              " 'What are they after?',\n",
              " 'I went swimming in the sea.',\n",
              " 'Mehmed Talat was assassinated in Berlin in 1921.',\n",
              " \"I ran all the way here and I'm out of breath.\",\n",
              " 'I can often hear pigeons cooing outside my bedroom window.',\n",
              " \"It didn't work.\",\n",
              " \"If I weren't sick, I'd join you.\",\n",
              " 'The work begins to get more difficult.',\n",
              " 'What does your father do?',\n",
              " 'I caught up with the others.',\n",
              " 'I was lonely.',\n",
              " 'Tom is very nice to me.',\n",
              " 'What makes you think that Tom is planning to ask Mary to marry him?',\n",
              " 'He is always laughing.',\n",
              " 'The station is dead ahead.',\n",
              " \"Tom apologized for not doing what he had promised he'd do.\",\n",
              " 'Tom arrived by car.',\n",
              " \"I'm kind of happy for you.\",\n",
              " 'How much did you pay?',\n",
              " 'The door is closing.',\n",
              " \"I know what I'm saying.\",\n",
              " 'My mother makes the best cakes in the world.',\n",
              " 'There is only one thing to do.',\n",
              " \"She's alive! She was drowning, but her father saved her.\",\n",
              " 'Take over.',\n",
              " 'We agree.',\n",
              " 'What did Tom give you for Christmas?',\n",
              " 'I despise Tom.',\n",
              " \"Because some urgent business came up, he wasn't able to go to the concert.\",\n",
              " \"Who's taking responsibility for this problem?\",\n",
              " 'You shall not marry my daughter!',\n",
              " 'Honk the horn.',\n",
              " 'Your message has been received.',\n",
              " ...)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences_en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "hr6GoqdzuVq9",
        "outputId": "e1f7e39f-e433-47db-8efd-b6780042740b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tenemos 118964 sentencias para entrenar\n",
            "Distribuciones del corpus en inglés\n",
            "count    118964.000000\n",
            "mean          6.310363\n",
            "std           2.611586\n",
            "min           1.000000\n",
            "25%           4.000000\n",
            "50%           6.000000\n",
            "75%           8.000000\n",
            "max          47.000000\n",
            "dtype: float64\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(f\"Tenemos {len(sentences_en)} sentencias para entrenar\")\n",
        "print(\"Distribuciones del corpus en inglés\")\n",
        "series_en = pd.Series([len(sentencia.split()) for sentencia in sentences_en])\n",
        "series_en.hist();\n",
        "print(series_en.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "pPbLpRYru4F3",
        "outputId": "bf0e9471-ac8d-4668-ca28-21f00424b04e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distribuciones del corpus en español\n",
            "count    118964.000000\n",
            "mean          6.083866\n",
            "std           2.764452\n",
            "min           1.000000\n",
            "25%           4.000000\n",
            "50%           6.000000\n",
            "75%           7.000000\n",
            "max          49.000000\n",
            "dtype: float64\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"Distribuciones del corpus en español\")\n",
        "series_es = pd.Series([len(sentencia.split()) for sentencia in sentences_es])\n",
        "series_es.hist();\n",
        "print(series_es.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wiqv5UaVrtMR",
        "outputId": "0fdbb3e4-bf71-4182-e31b-fe1d261bb30a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No one believed me.(4), '=>', Nadie me creía.(3)\n",
            "I wonder what happened to her.(6), '=>', Me pregunto qué le pasó a ella.(7)\n",
            "At last, we arrived at the village.(7), '=>', Al fin llegamos a la villa.(6)\n"
          ]
        }
      ],
      "source": [
        "origen = randint(0,len(sentences_es)-3)\n",
        "for i in range(origen, origen+3):\n",
        "    print(f\"{sentences_en[i]}({len(sentences_en[i].split())}), '=>', {sentences_es[i]}({len(sentences_es[i].split())})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INrVVj9_rtMR"
      },
      "source": [
        "### ENCODER-DECODER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_pkKlb_rtMR"
      },
      "source": [
        "En la sesión de redes recurrentes ya vimos la estructura básica y citamos algún uso de la misma  \n",
        "\n",
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/encoder_decoder.jpg?raw=1\" alt=\"Diagram of encoder_decoder\" width=\"400\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K6q_NP-rtMR"
      },
      "source": [
        "Y, ¿por qué esta arquitectura? Porque antes de que se propusiese no había forma de entrenar modelos que admitiesen secuencias de longitud variable recibiendo como target otra secuencia de longitud variable (y por tanto pudiendo ser esa longitud diferente a la primera)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGjjpK26rtMR"
      },
      "source": [
        "El encoder ahora se encarga de convertir cualquier secuencia que haya a la entrada en un vector de longitud fija y el decoder convertira este vector en una secuencia de salida de longitud variable.  \n",
        "\n",
        "De hecho al encoder le vamos a dar de comer secuencias de longitud fija pero lo suficientemente larga como para que entren todas, y aplicaremos el truco del padding para completar y el de la máscara para que no le afecte a las secuencias cortas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__amabJsrtMS"
      },
      "source": [
        "Este es el modelo que vamos a construir (sin \"desenrrollar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLiJdIRZrtMS"
      },
      "source": [
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/encoder_decoder_to_train.jpg?raw=1\" alt=\"Diagram of encoder_decoder\" width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYk-xYBOrtMS"
      },
      "source": [
        "Mejor si lo desenrrollamos:\n",
        "\n",
        "<img src=\"https://github.com/rodolso/DS_Online_Octubre24/blob/main/05_Deep_Learning/Sprint_19/Unidad_02_IA_Generativa_NLP_y_Texto/img/encoder_decoder_unrolled.jpg?raw=1\" alt=\"Diagram of encoder_decoder\" width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpCMaENGrtMS"
      },
      "source": [
        "Veamos como funcionaría (en entrenamiento) [*nota: h y c son los hidden_state de la recurrente del encoder, $h_d$ y $c_d$ son los hidden_state de la recurrente del decoder*]:\n",
        "1. Al enconder le damos la secuencia [I, like, soccer], y no le va a pasar nada todavía al encoder...\n",
        "2. Hace el embedding, supongamos que de 2 dimensiones, [ (0.212,-3.32), (1.34, 0.344), (6.665,-4.443)]\n",
        "3. Procesa la secuencia uno a uno y va transmitiéndose el hidden_state (y la cell_state, es una LSTM) en cada elemento de la secuencia:  \n",
        "    > Procesa e0: [(0.212,-3.32),(0,0,...0),(0,0.....)]  \n",
        "    > Procesa e1: [(1.34,0.344), h([(0.212,-3.32),(0,0,...0),(0,0.....)]),c((0.212,-3.32),(0,0,...0),(0,0.....))] (recordad que las LSTM tienen dos estados ocultos h y c el primero en teoría para la memoria a corto y el segundo para la memoria a largo)  \n",
        "    > Procesa e2 [(6.665, -4.443), h(e1), c(1)]  \n",
        "4. Ahora sí devuelve [salida(e2),h(e2),c(2)] y esto es parte de lo que entra en el Decoder\n",
        "5. El decoder a la vez ha hecho el embedding de su entrada [emb(\"\\<sos\\>\"),emb(\"Me\"),emb(\"gusta\"),emb(\"el\"),emb(\"fútbol\")]\n",
        "5. Lo primero que procesa el decoder es d1: [h(e2),c(e2),emb(\"\\<sos\\>\")] y la capa de salida predice (en el caso de la figura) \"me\"  \n",
        "6. Luego procesa d2: [emb(\"Me\"),$h_d$(d1),$c_d$(d1)] y la capa de salida predice (en este caso): \"encanta\"\n",
        "7. procesa d3: [emb(\"gusta\"),$h_d$(d2),$c_d$(d2)] y la capa de salida predice: \"el\" (Importante, le entra el embedding de la palabra que tendría que haber predicho antes (\"gusta\") no la que realmente predijo \"encanta\", esto es *Teaching Forcing*)\n",
        "8. procesa d4: [emb(\"el\"),$h_d$(d3),$c_d$(d3)] y la capa de salida predice: \"fútbol\"\n",
        "9. procesa d5: [emb(\"fútbol\"),$h_d$(d4),$c_d$(4)] y la capa de salida predice: \"\\<eos\\>\" (end of sequence) (podría haber hecho la predicción de otra palabra y hubiera acabado igual pero se contabilizaría como un error para el optimizador, etc, etc)\n",
        "10. Se acaba la secuencia de entrada para el decoder\n",
        "\n",
        "A destacar:\n",
        "- El encoder sólo le pasara los hidden_state (h y c) del final de la secuencia de entrada al decoder\n",
        "- El decoder trabaja sobre el target completo desplazado una vez (esto nos sirve para construir el vec2seq)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8ADOrEjrtMS"
      },
      "source": [
        "### Construcción del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-mpvJODrtMS"
      },
      "source": [
        "Ok, ahora que todo ha quedado clarito como la teoría de la relatividad, vamos a construir el modelo.  \n",
        "\n",
        "Primero las capas de embeddings: como ya hemos visto primero nuestros vectorizadores para convertir cada sentencia en secuencia de índices y después la capa de embedding para que aprenda cuál es la mejor reprensentación de cada índice/palabra en el contexto del problema que estamos resolviendo. (De hecho, ***inciso: ¿qué es lo que realmente está haciendo el encoder...***, *se te ocurre qué podríamos hacer con el encoder una vez entrenado todo el modelo...*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "luUMMVZ0rtMS"
      },
      "outputs": [],
      "source": [
        "vocab_size = 5000 # Número de tokens de nuestro vocabulario, en este caso vamos a hacer que token = (conjunto caracteres separados por espacios)\n",
        "max_length = 50 # Las secuencias de entrada están fijadas a 50, podríamos haberlas fijado a...\n",
        "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length=max_length) # Como no decimos nada split=\"whitespace\", o sea la tokenizacion mencionada\n",
        "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length=max_length)\n",
        "\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es]) # Importante le añadimos el comienzo de secuencia y el final para que sepa donde empieza y para que aprenda cuando se acaba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxiguIiTrtMT",
        "outputId": "62712792-650c-4b0e-973b-84ad8acade5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_vec_layer_en.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRDoz0NOrtMT",
        "outputId": "f9284869-d804-4a86-85f2-24cdabdb9e43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['', '[UNK]', 'startofseq', 'endofseq', 'de', 'que', 'a', 'no', 'tom', 'la']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_vec_layer_es.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKXkImlH5Sdd"
      },
      "source": [
        "Veamos cómo codifica algunas de las setencias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH1Nz_po5iV4",
        "outputId": "e0d46ba9-485b-47f6-c0ea-43b030b4d974"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tom is brain damaged.(4), '=>', Tom sufre de daño cerebral.(5)\n",
            "Vectorizacion sin embedding de la entrada al encoder\n",
            "tf.Tensor(\n",
            "[   6    8 2897 2653    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0], shape=(50,), dtype=int64)\n",
            "Vectorizacion sin embedding de la entrada al decoder\n",
            "tf.Tensor(\n",
            "[   2    8 3941    4 1030    1    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0], shape=(50,), dtype=int64)\n",
            "Vectorizacion del target\n",
            "tf.Tensor(\n",
            "[   8 3941    4 1030    1    3    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0], shape=(50,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "origen = randint(0,len(sentences_es)-3)\n",
        "for i in range(origen, origen+1):\n",
        "    print(f\"{sentences_en[i]}({len(sentences_en[i].split())}), '=>', {sentences_es[i]}({len(sentences_es[i].split())})\")\n",
        "    print(\"Vectorizacion sin embedding de la entrada al encoder\", text_vec_layer_en(sentences_en[i]), sep = \"\\n\")\n",
        "    print(\"Vectorizacion sin embedding de la entrada al decoder\", text_vec_layer_es(f\"startofseq {sentences_es[i]}\"), sep = \"\\n\")\n",
        "    print(\"Vectorizacion del target\", text_vec_layer_es(f\"{sentences_es[i]} endofseq\"), sep = \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGVInDEhrtMT"
      },
      "source": [
        "Construimos los datasets de entrenamiento y validación, teniendo en cuenta que encoder y decoder reciben entradas ligeramente diferentes y que el target debe contener el __endofseq__ (que es un token que debe predecir el modelo, es decir debe predecir cuando acaba la frase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6-vrcZDyrtMT"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'text_vec_layer_es' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13600\\1328610107.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_train_dec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf\"startofseq {s}\"\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences_es\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100_000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_valid_dec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf\"startofseq {s}\"\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences_es\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100_000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mY_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_vec_layer_es\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf\"{s} endofseq\"\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences_es\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100_000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mY_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_vec_layer_es\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf\"{s} endofseq\"\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences_es\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100_000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'text_vec_layer_es' is not defined"
          ]
        }
      ],
      "source": [
        "X_train = tf.constant(sentences_en[:100_000])\n",
        "X_valid = tf.constant(sentences_en[100_000:])\n",
        "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
        "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
        "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
        "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(100000,), dtype=string, numpy=\n",
              "array([b'startofseq Qu\\xc3\\xa9 aburrimiento!',\n",
              "       b'startofseq Adoro el deporte.',\n",
              "       b'startofseq Te gustar\\xc3\\xada que intercambiemos los trabajos?',\n",
              "       ..., b'startofseq Tom est\\xc3\\xa1 en el tel\\xc3\\xa9fono.',\n",
              "       b'startofseq Traemos estupendas noticias!',\n",
              "       b'startofseq Este es un ejemplo de la ley del m\\xc3\\xa1s fuerte, como se le suele llamar.'],\n",
              "      dtype=object)>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_dec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6XplSzgrtMT"
      },
      "source": [
        "Y ahora sí, comenzamos con la definición (funcional del modelo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "tB_pShS-rtMT"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdCZZOiSrtMU"
      },
      "outputs": [],
      "source": [
        "embed_size = 128\n",
        "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
        "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
        "\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        " \n",
        "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "O6GgJ1xrrtMU"
      },
      "outputs": [],
      "source": [
        "encoder = tf.keras.layers.LSTM(512, return_state=True)\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings) # IMPORTANTE obtenemos los estados de salida del encoder que es lo que...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BIHHLh7prtMU"
      },
      "outputs": [],
      "source": [
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state) # ... realmente vamos a pasar al decoder para el primer token (<sos>) de la secuencia de guía (que en el entrenamiento es la de target desplazada)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FVaTrH1TrtMU"
      },
      "outputs": [],
      "source": [
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\") # La salida es una softmax con tantas neuronas como términos en el vocabulario, es decir estamos prediciendo el índice de cada palabra de la respuesta. Luego tendremos que decodificarlo para obtener la palabra real\n",
        "Y_proba = output_layer(decoder_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-W99DourtMW"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly a couple hours if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZT7TWtjrtMW",
        "outputId": "1b1cdf46-9487-4ad1-8767-0a7ece82f782"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3125/3125 [==============================] - 146s 45ms/step - loss: 0.5719 - accuracy: 0.3169 - val_loss: 0.4419 - val_accuracy: 0.4184\n",
            "Epoch 2/10\n",
            "3125/3125 [==============================] - 142s 45ms/step - loss: 0.3769 - accuracy: 0.4756 - val_loss: 0.3393 - val_accuracy: 0.5168\n",
            "Epoch 3/10\n",
            "3125/3125 [==============================] - 142s 46ms/step - loss: 0.2852 - accuracy: 0.5648 - val_loss: 0.2887 - val_accuracy: 0.5731\n",
            "Epoch 4/10\n",
            "3125/3125 [==============================] - 142s 46ms/step - loss: 0.2274 - accuracy: 0.6306 - val_loss: 0.2633 - val_accuracy: 0.6046\n",
            "Epoch 5/10\n",
            "3125/3125 [==============================] - 146s 47ms/step - loss: 0.1871 - accuracy: 0.6811 - val_loss: 0.2508 - val_accuracy: 0.6210\n",
            "Epoch 6/10\n",
            "3125/3125 [==============================] - 147s 47ms/step - loss: 0.1567 - accuracy: 0.7233 - val_loss: 0.2452 - val_accuracy: 0.6303\n",
            "Epoch 7/10\n",
            "3125/3125 [==============================] - 147s 47ms/step - loss: 0.1325 - accuracy: 0.7587 - val_loss: 0.2449 - val_accuracy: 0.6360\n",
            "Epoch 8/10\n",
            "3125/3125 [==============================] - 146s 47ms/step - loss: 0.1128 - accuracy: 0.7902 - val_loss: 0.2490 - val_accuracy: 0.6380\n",
            "Epoch 9/10\n",
            "3125/3125 [==============================] - 146s 47ms/step - loss: 0.0962 - accuracy: 0.8181 - val_loss: 0.2553 - val_accuracy: 0.6356\n",
            "Epoch 10/10\n",
            "3125/3125 [==============================] - 145s 46ms/step - loss: 0.0827 - accuracy: 0.8415 - val_loss: 0.2628 - val_accuracy: 0.6356\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x262a302ff10>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPtEaBMqrtMW"
      },
      "source": [
        "Una vez entrenado el modelo, la traducción tiene un poco de miga.   \n",
        "\n",
        "El decoder espera que le pasemos una secuencia guía (el teacher), que es la función que hacía la secuencia target desplazada uno en el entrenamiento.  \n",
        "\n",
        "Lo que vamos a hacer es ir prediciendo palabra a palabra introduciendo como guía la última predicción hasta llegar a que el modelo devuelva el carácter de fin de secuencia y en ese momento devolvemos la \"traducción\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "adTniqXUrtMW"
      },
      "outputs": [],
      "source": [
        "def translate(sentence_en):\n",
        "    translation = \"\"\n",
        "    for word_idx in range(max_length):\n",
        "        X = np.array([sentence_en]).astype(object)  # encoder input\n",
        "        X_dec = np.array([\"startofseq \" + translation]).astype(object)  # decoder input\n",
        "\n",
        "        y_probs = model.predict((X, X_dec))\n",
        "        y_proba = y_probs[0, word_idx]  # last token's probas\n",
        "\n",
        "        predicted_word_id = np.argmax(y_proba)\n",
        "        predicted_proba = round(float(y_proba[predicted_word_id]),3)\n",
        "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
        "        \n",
        "        if predicted_word == \"endofseq\":\n",
        "            break\n",
        "        translation += \" \" + predicted_word\n",
        "        print(f\"{translation}({predicted_proba})\")\n",
        "    return translation.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnh_m6khrtMW"
      },
      "source": [
        "Probemos con algo sencillo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "CKivIVBkrtMW",
        "outputId": "4c7ef2f5-969b-4538-ec50-02fce75d8bf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            " me(0.993)\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            " me gusta(0.999)\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            " me gusta el(0.975)\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            " me gusta el fútbol(0.978)\n",
            "1/1 [==============================] - 0s 51ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'me gusta el fútbol'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"I like soccer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDJfrl0mNeaS"
      },
      "source": [
        "Y si cambiamos un poco las palabras...:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "G27He7G0NdPh",
        "outputId": "4c6c5a20-f48d-4617-f428-d1b135109e1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 53ms/step\n",
            " me(0.991)\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            " me encanta(0.889)\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            " me encanta jugar(0.92)\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            " me encanta jugar al(0.878)\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            " me encanta jugar al fútbol(0.994)\n",
            "1/1 [==============================] - 0s 51ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'me encanta jugar al fútbol'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"I love playing football\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UU9czJjrtMW"
      },
      "source": [
        "Bien!!! Pero qué ocurre si le pedimos algo más largo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "sbpR3btrrtMX",
        "outputId": "987611f9-be66-40e8-930e-70d40486b57c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 57ms/step\n",
            " me(0.952)\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            " me gusta(0.999)\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            " me gusta la(0.796)\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            " me gusta la cerveza(0.355)\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            " me gusta la cerveza que(0.421)\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            " me gusta la cerveza que tengo(0.343)\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            " me gusta la cerveza que tengo que(0.513)\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            " me gusta la cerveza que tengo que hacer(0.564)\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            " me gusta la cerveza que tengo que hacer el(0.502)\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            " me gusta la cerveza que tengo que hacer el metro(0.812)\n",
            "1/1 [==============================] - 0s 62ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'me gusta la cerveza que tengo que hacer el metro'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"I like soccer and also going to the beach\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ov8c7UzrtMX"
      },
      "source": [
        "Vamos a ver mejoras que además nos vayan adelantando conceptos para llegar a los LLN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RreDpT8rtMX"
      },
      "source": [
        "## Bidirectional RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zudtQcm1rtMX"
      },
      "source": [
        "Una red recurrente bidireccional es la que lee la secuencia tanto de izquierda a derecha como de derecha a izquierda y procesa ambas secuencias en conjunto. Ojo: la secuencia de entrada.\n",
        "\n",
        "En general es como tener una capa que mira en un sentido y otra en el otro y concatenar luego sus salidas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSNm3wlTrtMX"
      },
      "source": [
        "<img src=\"https://github.com/rodolso/DS_Online_Octubre24/blob/main/05_Deep_Learning/Sprint_19/Unidad_02_IA_Generativa_NLP_y_Texto/img/bidirectionalrnn.jpg?raw=1\" alt=\"Bidirectional RNN\" width=\"700\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzxTTMFzrtMX"
      },
      "source": [
        "¿Por qué y para qué? Porque, por ejemplo, hay frases que para traducirlas necesitas ver que viene después, como en el caso de los adjetivos en inglés que anteceden al nombre y de los sinónimos en un idioma que no coinciden necesariamente con los sinónimos en otro: the left arm, they left party, they left the restaurant...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kdGZV5FrtMX"
      },
      "source": [
        "Para crear un capa recurrente bidireccional, se hace lo siguiente (encapsular una recurrente en una más genérica denominada Bidirectional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "a4fFw2scrtMY"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "encoder = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_state=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrN_tlVyrtMY"
      },
      "source": [
        "En el caso del decoder no podemos hacer lo mismo porque en el target mirar al futuro sí es hacer trampa (recordemos que hasta ahora le pasamos la palabra que correspondería a la palabra esperada anterior, o sea no hacemos trampa), y por tanto no serviría para predecir algo que no hubiera visto (de hecho no podríamos construir una entrada para predecir de forma correcta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jle372OvrtMY"
      },
      "source": [
        "Pero, la recurrente bidireccional produce el doble de estados ocultos. Como se trata de una LSTM tendremos dos hidden_state y dos cell_state, aunque el decoder sólo espera dos (porque es otra LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6UyFduHrtMY"
      },
      "source": [
        "Lo que hacemos es concatenarlos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "5dIrmT73V-BU"
      },
      "outputs": [],
      "source": [
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "# Wrap tf.concat within a Lambda layer to make it compatible with KerasTensors\n",
        "encoder_state = [tf.keras.layers.Lambda(lambda x: tf.concat(x[::2], axis=-1))(encoder_state),  # short-term (0 & 2)\n",
        "                 tf.keras.layers.Lambda(lambda x: tf.concat(x[1::2], axis=-1))(encoder_state)]  # long-term (1 & 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "532Ve8sgrtMb"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly a couple hours if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZadV-ZArtMc",
        "outputId": "c1c4d625-1767-4ccf-b43c-e88b9c9724af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3125/3125 [==============================] - 163s 50ms/step - loss: 0.4434 - accuracy: 0.4446 - val_loss: 0.3086 - val_accuracy: 0.5589\n",
            "Epoch 2/10\n",
            "3125/3125 [==============================] - 174s 56ms/step - loss: 0.2484 - accuracy: 0.6197 - val_loss: 0.2434 - val_accuracy: 0.6278\n",
            "Epoch 3/10\n",
            "3125/3125 [==============================] - 178s 57ms/step - loss: 0.1862 - accuracy: 0.6905 - val_loss: 0.2212 - val_accuracy: 0.6559\n",
            "Epoch 4/10\n",
            "3125/3125 [==============================] - 184s 59ms/step - loss: 0.1509 - accuracy: 0.7360 - val_loss: 0.2152 - val_accuracy: 0.6644\n",
            "Epoch 5/10\n",
            "3125/3125 [==============================] - 172s 55ms/step - loss: 0.1258 - accuracy: 0.7718 - val_loss: 0.2152 - val_accuracy: 0.6686\n",
            "Epoch 6/10\n",
            "3125/3125 [==============================] - 185s 59ms/step - loss: 0.1064 - accuracy: 0.8017 - val_loss: 0.2182 - val_accuracy: 0.6673\n",
            "Epoch 7/10\n",
            "3125/3125 [==============================] - 183s 59ms/step - loss: 0.0908 - accuracy: 0.8264 - val_loss: 0.2245 - val_accuracy: 0.6666\n",
            "Epoch 8/10\n",
            "3125/3125 [==============================] - 174s 56ms/step - loss: 0.0780 - accuracy: 0.8493 - val_loss: 0.2309 - val_accuracy: 0.6663\n",
            "Epoch 9/10\n",
            "3125/3125 [==============================] - 167s 53ms/step - loss: 0.0677 - accuracy: 0.8674 - val_loss: 0.2397 - val_accuracy: 0.6623\n",
            "Epoch 10/10\n",
            "3125/3125 [==============================] - 162s 52ms/step - loss: 0.0594 - accuracy: 0.8823 - val_loss: 0.2471 - val_accuracy: 0.6621\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x262a19ae640>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# extra code — completes the model and trains it\n",
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
        "\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "Y_proba = output_layer(decoder_outputs)\n",
        "\n",
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "qUG81j9YrtMc",
        "outputId": "1bab5b0f-0661-49e7-e9ad-3811f903b14a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 4s 4s/step\n",
            " me(0.983)\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            " me gusta(1.0)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta el(0.999)\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            " me gusta el [UNK](0.531)\n",
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'me gusta el [UNK]'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"I like soccer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "-NF2SQovUKdb",
        "outputId": "0c032648-2331-4246-e0c6-efacc760a10e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 26ms/step\n",
            " me(0.921)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta(0.993)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta el(0.606)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta el fútbol(0.339)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta el fútbol y(1.0)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta el fútbol y a(0.231)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            " me gusta el fútbol y a la(0.589)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta el fútbol y a la playa(0.982)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta el fútbol y a la playa también(0.883)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me gusta el fútbol y a la playa también [UNK](0.296)\n",
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'me gusta el fútbol y a la playa también [UNK]'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"I like soccer and also going to the beach\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-PZrYYArtMc"
      },
      "source": [
        "Otra posible optimización es lo que se denomina Beam Search. Se deja a modo de ejercicio para entenderlo y una referencia explicativa:\n",
        "https://towardsdatascience.com/an-intuitive-explanation-of-beam-search-9b1d744e7a0f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzKUaOHortMc"
      },
      "source": [
        "## Beam Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDpeeJ1ArtMc"
      },
      "source": [
        "This is a very basic implementation of beam search. I tried to make it readable and understandable, but it's definitely not optimized for speed! The function first uses the model to find the top _k_ words to start the translations (where _k_ is the beam width). For each of the top _k_ translations, it evaluates the conditional probabilities of all possible words it could add to that translation. These extended translations and their probabilities are added to the list of candidates. Once we've gone through all top _k_ translations and all words that could complete them, we keep only the top _k_ candidates with the highest probability, and we iterate over and over until they all finish with an EOS token. The top translation is then returned (after removing its EOS token).\n",
        "\n",
        "* Note: If p(S) is the probability of sentence S, and p(W|S) is the conditional probability of the word W given that the translation starts with S, then the probability of the sentence S' = concat(S, W) is p(S') = p(S) * p(W|S). As we add more words, the probability gets smaller and smaller. To avoid the risk of it getting too small, which could cause floating point precision errors, the function keeps track of log probabilities instead of probabilities: recall that log(a\\*b) = log(a) + log(b), therefore log(p(S')) = log(p(S)) + log(p(W|S))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "mi1xCiRf_cif"
      },
      "outputs": [],
      "source": [
        "# extra code – a basic implementation of beam search\n",
        "\n",
        "def beam_search(sentence_en, beam_width, verbose=False):\n",
        "    X = np.array([sentence_en]).astype(object)  # encoder input\n",
        "    X_dec = np.array([\"startofseq\"]).astype(object)  # decoder input\n",
        "    y_proba = model.predict((X, X_dec))[0, 0]  # first token's probas\n",
        "    top_k = tf.math.top_k(y_proba, k=beam_width)\n",
        "    top_translations = [  # list of best (log_proba, translation)\n",
        "        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])\n",
        "        for word_proba, word_id in zip(top_k.values, top_k.indices)\n",
        "    ]\n",
        "\n",
        "    # extra code – displays the top first words in verbose mode\n",
        "    if verbose:\n",
        "        print(\"Top first words:\", top_translations)\n",
        "\n",
        "    for idx in range(1, max_length):\n",
        "        candidates = []\n",
        "        for log_proba, translation in top_translations:\n",
        "            if translation.endswith(\"endofseq\"):\n",
        "                candidates.append((log_proba, translation))\n",
        "                continue  # translation is finished, so don't try to extend it\n",
        "            X = np.array([sentence_en]).astype(object)  # encoder input\n",
        "            X_dec = np.array([\"startofseq \" + translation]).astype(object)  # decoder input\n",
        "            y_proba = model.predict((X, X_dec))[0, idx]  # last token's proba\n",
        "            for word_id, word_proba in enumerate(y_proba):\n",
        "                word = text_vec_layer_es.get_vocabulary()[word_id]\n",
        "                candidates.append((log_proba + np.log(word_proba),\n",
        "                                   f\"{translation} {word}\"))\n",
        "        top_translations = sorted(candidates, reverse=True)[:beam_width]\n",
        "\n",
        "        # extra code – displays the top translation so far in verbose mode\n",
        "        if verbose:\n",
        "            print(\"Top translations so far:\", top_translations)\n",
        "\n",
        "        if all([tr.endswith(\"endofseq\") for _, tr in top_translations]):\n",
        "            return top_translations[0][1].replace(\"endofseq\", \"\").strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "jndVFH6qrtMd",
        "outputId": "174e7e62-2c1b-4515-fcb5-d04c53101815"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n",
            " me(0.99)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me encantan(0.958)\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            " me encantan los(0.808)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me encantan los perros(0.92)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me encantan los perros y(0.914)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me encantan los perros y a(0.516)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " me encantan los perros y a mí(0.468)\n",
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'me encantan los perros y a mí'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# extra code – shows how the model making an error\n",
        "sentence_en = \"I love cats and dogs\"\n",
        "translate(sentence_en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgMfG50S9p9A"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly half an hour using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "7eKZpXLortMd"
      },
      "outputs": [],
      "source": [
        "# # extra code – shows how beam search can help\n",
        "# beam_search(sentence_en, beam_width=3, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHZA7cMKrtMd"
      },
      "source": [
        "The correct translation is in the top 3 sentences found by beam search, but it's not the first. Since we're using a small vocabulary, the \\[UNK] token is quite frequent, so you may want to penalize it (e.g., divide its probability by 2 in the beam search function): this will discourage beam search from using it too much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNF-QP37rtMd"
      },
      "source": [
        "## Mecanismos de Atencion (Attention mechanisms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52TxgFCRrtMd"
      },
      "source": [
        "Como mejora a este tipo de arquitecturas, en 2014 (Dzmitry Bahdanau y colegas, et al. que se dice) introdujeron una mejora sustancial a la arquitectura de Encoder-Decoders.\n",
        "\n",
        "La idea detrás del mecanismo es pasarle al decoder más información de la secuencia de entrada y no sólo los estados ocultos producidos por el último elemento (el primero y el úlitmo en el caso de bidireccionales). ¿Qué información? Pues algo así como la palabra que más le aporte en cada momento. Por ejemplo que cuando al decoder le toque producir fútbol en la traducción de I like soccer, reciba \"soccer\" (en concreto la salida del encoder a la palabra \"soccer\").  \n",
        "\n",
        "Supongamos que estamos traduciendo frases como:\n",
        "\n",
        "- I like soccer    \n",
        "- I like Rain Man  \n",
        "- you like The Bridge  \n",
        "- we like Jaime  \n",
        "  \n",
        "Para los dos primeras el decoder iría traduciendo:\n",
        "(Me) gusta ... y la idea es que las entradas \"soccer\", \"Rain\" + \"Man\", \"The\" + \"Bridge\", \"Jaime\" aporten más en ese instante...  \n",
        "\n",
        "Entonces al decoder tendré que pasarle todas las palabras de la frase (en concreto la salida de cada una de estas del encoder) y que exista un mecanismo que le diga en función de lo que lleva cuál de las entradas debe considerar más (aquí nos fijamos en la siguiente, pero para traducir \"Me\" es mejor que se fije en la primera, el pronombre, para traducir \"gusta\", igual)  \n",
        "\n",
        "Lo interesante no es considerar solo una palabra de entrada, sino una combinación pesada de estas (una combinación lineal -> \"The\" + \"Bridge\")\n",
        "\n",
        "Es decir, volviendo a nuestras entradas del decoder:\n",
        "\n",
        "* Antes d1: [h(e2),c(2),emb(\"<start_of_sequence>\")], ahora da1 (la a es de attention): [h(e2),c(e2), (coef1 * e1 + coef2 * e2 + coef3 * e3)] y probablemente todos los coef se aproximarían a cero\n",
        "* Antes d2: [emb(\"Me\"),$h_d$(d1),$c_d$(d1)], ahora da2: [emb(\"Me\"),$h_d$(d1),$c_d$(d1), (coef21 * e1 + coef22 * e2 + coef23 * e3)] y probablemente coef21 >> coef22 y coef23\n",
        "* Antes d3: [emb(\"gusta\"),$h_d$(d2),$c_d$(d2)], ahora da3: [emb(\"gusta\"),$h_d$(d1),$c_d$(d1), (coef31 * e1 + coef32 * e2 + coef33 * e3) ] y coef31 y coef32 serán altos y coef33 bajo o nulo\n",
        "\n",
        "\n",
        "__¿Y cómo le digo en cuál debe fijarse más? Pues como en los embeddings... que lo aprenda :-) (o sea tendré una Attention Layer)__\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSJSezrertMd"
      },
      "source": [
        "Muy bien, y cómo se hace ese \"que lo aprenda\": Dos mecanismos de Atencion (aditiva y multiplicativa), pero el multiplicativo ha superado al aditivo y de hecho la Attention Layer de keras hace Attention multiplicativa y además al final la predicción se hace a partir de la salida de la capa de atención.\n",
        "\n",
        "Gráficamente:\n",
        "\n",
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/encoder_decoder_with_attention.jpg?raw=1\" alt=\"Encoder_Decoder with Attention\" width=\"700\" />\n",
        "\n",
        "\n",
        "Intuitivamente al poner la capa de atención al final está configurando toda la red (toda incluidos los embeddings) para que \"memorice\" la relación estadística entre las posiciones de salida y las de entrada en diferentes situaciones. Y luego ya nosotros a eso le llamamos atención, porque es verdad que cuando llega el momento de \"Me gusta el...\", ha memorizado que las posiciones que deben aportar más es donde haya salidas que generalmente pertenecen a nombres. (pero él ni sabe que son nombres, ni que está traduciendo ni nada,...)  \n",
        "\n",
        "\n",
        "El siguiente paso sería aumentar la memoria y olvidarse de las recurrencias (y del problema de traducir) :-)... los Transformers, pero antes... prestémosle Atención al traductor con atención"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "path = tf.keras.utils.get_file(\"spa-eng.zip\",\n",
        "                               origin=url,\n",
        "                               cache_dir=\"datasets\",\n",
        "                               extract=True)\n",
        "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text(encoding = \"utf-8\")\n",
        "\n",
        "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
        "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
        "np.random.seed(42)  # extra code – ensures reproducibility on CPU\n",
        "np.random.shuffle(pairs)\n",
        "sentences_en, sentences_es = zip(*pairs)  # separa las parejas en dos listas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = 5000 # Número de tokens de nuestro vocabulario, en este caso vamos a hacer que token = (conjunto caracteres separados por espacios)\n",
        "max_length = 50 # Las secuencias de entrada están fijadas a 50, podríamos haberlas fijado a...\n",
        "\n",
        "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length=max_length) # Como no decimos nada split=\"whitespace\", o sea la tokenizacion mencionada\n",
        "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length=max_length)\n",
        "\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es]) # Importante le añadimos el comienzo de secuencia y el final para que sepa donde empieza y para que aprenda cuando se acaba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = tf.constant(sentences_en[:100_000])\n",
        "X_valid = tf.constant(sentences_en[100_000:])\n",
        "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
        "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
        "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
        "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "embed_size = 128\n",
        "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
        "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
        "\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        " \n",
        "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6sxAEvslrtMd"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "encoder = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True)) # Ahora necesitamos todas las salidas del Encoder por eso return_sequences = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL9ZaWurrtMe"
      },
      "outputs": [],
      "source": [
        "# extra code – this part of the model is exactly the same as earlier\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "# Wrap tf.concat within a Lambda layer to make it compatible with KerasTensors\n",
        "encoder_state = [tf.keras.layers.Lambda(lambda x: tf.concat(x[::2], axis=-1))(encoder_state),  # short-term (0 & 2)\n",
        "                 tf.keras.layers.Lambda(lambda x: tf.concat(x[1::2], axis=-1))(encoder_state)]  # long-term (1 & 3)\n",
        "\n",
        "\n",
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5v4cs87rtMe"
      },
      "source": [
        "And finally, let's add the `Attention` layer and the output layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6CKhAJ5wgEvW"
      },
      "outputs": [],
      "source": [
        "attention_layer = tf.keras.layers.Attention()\n",
        "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "Y_proba = output_layer(attention_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncoE2lSKrtMe"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly a couple hours if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aICWvCoBrtMe",
        "outputId": "8c0b008e-6094-4f78-a895-da6c92cc5c4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3125/3125 [==============================] - 173s 53ms/step - loss: 0.6102 - accuracy: 0.3034 - val_loss: 0.4634 - val_accuracy: 0.4206\n",
            "Epoch 2/10\n",
            "3125/3125 [==============================] - 168s 54ms/step - loss: 0.3805 - accuracy: 0.4991 - val_loss: 0.3289 - val_accuracy: 0.5493\n",
            "Epoch 3/10\n",
            "3125/3125 [==============================] - 166s 53ms/step - loss: 0.2769 - accuracy: 0.5956 - val_loss: 0.2715 - val_accuracy: 0.6069\n",
            "Epoch 4/10\n",
            "3125/3125 [==============================] - 174s 56ms/step - loss: 0.2238 - accuracy: 0.6498 - val_loss: 0.2466 - val_accuracy: 0.6328\n",
            "Epoch 5/10\n",
            "3125/3125 [==============================] - 178s 57ms/step - loss: 0.1910 - accuracy: 0.6871 - val_loss: 0.2347 - val_accuracy: 0.6469\n",
            "Epoch 6/10\n",
            "3125/3125 [==============================] - 180s 58ms/step - loss: 0.1679 - accuracy: 0.7154 - val_loss: 0.2293 - val_accuracy: 0.6529\n",
            "Epoch 7/10\n",
            "3125/3125 [==============================] - 183s 58ms/step - loss: 0.1497 - accuracy: 0.7390 - val_loss: 0.2282 - val_accuracy: 0.6589\n",
            "Epoch 8/10\n",
            "3125/3125 [==============================] - 185s 59ms/step - loss: 0.1347 - accuracy: 0.7591 - val_loss: 0.2288 - val_accuracy: 0.6626\n",
            "Epoch 9/10\n",
            "3125/3125 [==============================] - 177s 57ms/step - loss: 0.1222 - accuracy: 0.7769 - val_loss: 0.2317 - val_accuracy: 0.6620\n",
            "Epoch 10/10\n",
            "3125/3125 [==============================] - 190s 61ms/step - loss: 0.1116 - accuracy: 0.7931 - val_loss: 0.2365 - val_accuracy: 0.6591\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1dfa2451c70>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "0gnOroYdrtMe",
        "outputId": "3af509e0-0dd8-4650-9bbb-c0975cd792e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            " me(0.967)\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            " me gusta(0.809)\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            " me gusta tus(0.523)\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            " me gusta tus zapatos(0.996)\n",
            "1/1 [==============================] - 0s 47ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'me gusta tus zapatos'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"I like your shoes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "q8upfzZJ3Bsc",
        "outputId": "e3feff4e-902a-4736-99ec-e1fdf4c42637"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 41ms/step\n",
            " te(0.811)\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            " te amo(0.412)\n",
            "1/1 [==============================] - 0s 39ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'te amo'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"I love you\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "ecVI36mR9zR2",
        "outputId": "2618b83b-7d94-439c-c6e1-013ea54fa998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 51ms/step\n",
            " me(0.777)\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            " me gusta(0.985)\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            " me gusta el(0.579)\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            " me gusta el fútbol(0.858)\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            " me gusta el fútbol y(0.996)\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            " me gusta el fútbol y también(0.312)\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            " me gusta el fútbol y también a(0.943)\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            " me gusta el fútbol y también a la(0.996)\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            " me gusta el fútbol y también a la playa(0.976)\n",
            "1/1 [==============================] - 0s 56ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'me gusta el fútbol y también a la playa'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate(\"I like soccer and also going to the beach\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DowPj8m1-NiO"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly half an hour using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "-rIi2lK7rtMe"
      },
      "outputs": [],
      "source": [
        "# beam_search(\"I like soccer and also going to the beach\", beam_width=3,\n",
        "#             verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx3jPpJOrtMf"
      },
      "source": [
        "## Y si la solución es simple y llanamente establecer relación los elementos de la secuencia entre sí... ***Attention Is All You Need: The Transformer Architecture***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eghXhvLprtMf"
      },
      "source": [
        "Y en 2017, alguien de una empresa..., publicó un paper en el que se presentaba en sociedad una arquitectura que sólo necesitaba de mecanismos de Atenttion y capas Densas para hacer NMT como el mejor.\n",
        "\n",
        "OJO, esta sección es a título ilustrativo... Pero molón\n",
        "\n",
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/transformers.webp?raw=1\" alt=\"Encoder_Decoder with Attention\" width=\"700\" />\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snQivuXHrtMf"
      },
      "source": [
        "Antes de asustarse, lo que realmente hace esta arquitectura es ser la \"superconvolucional\" con memoria de las secuencias de texto, pero vamos por partes:\n",
        "\n",
        "__La parte del encoder__ al incluir el mecanismo de atención (y ese positional encoding) lo que está haciendo es aprender las relaciones posicionales de las palabras del \"lenguaje\" de entrada (sí está memorizando y caracterizando las relaciones entre todas las palabras y sus posiciones de entrada), está haciendo un embedding de la información posicional. Luego concatena la información posicional con la de entrada y aprende a relacionarla lo mejor posible para esa secuencia. En tiempo de inferencia diríamos que para cada secuencia está analizándola sintácticamente, gramaticalmente, etc, etc y luego la devuelve... Pero nunca le hemos pasado información ni sintáctica, ni gramatical, ni nada.  \n",
        "\n",
        "__La parte del decoder__ primero hace lo mismo que el encoder pero con el \"lenguaje\" de destino, se aprende y caracteriza todas las relaciones posicionales de las sentencias de ese \"lenguaje\". Combina ese embedding por secuencia con la secuencia de destino y se lo pasa a la siguiente capa de atención que ahora memoriza y caracteriza todas las relaciones posicionales entre las sentencias procesadas y enriquecidas del lenguaje destino y las sentencias procesadas y enriquecidas del lenguaje de origen. Y luego el feedfoward es el que realmente mezcla todo (mezclará toda la información de la secuencia, vease que se va transmitiendo, las posiciones y las posiciones con el lenguaje origen).\n",
        "\n",
        "Para aumentar la memoria y dar más relaciones lo que hacemos es que la capa de atención en realidad son muchas capas de atención en paralelo y acumular modulos de atención (como acumulábamos capas convolucionales en una red convolucional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl91Sdu4rtMf"
      },
      "source": [
        "A título ilustrativo un __capa multihead de atención__:\n",
        "\n",
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/multihead_attention.png?raw=1\" alt=\"Multihead Attention Layer\" width=\"700\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh_6C3mWrtMf"
      },
      "source": [
        "Las capas linear son capas densas sin función de activación, tienen pesos entrenables -> Memoria (en estos pesos está la memoria de las características posicionales)\n",
        "Y las capas head fuerzan que las linear (tantas como cabezas*3) aprendan un número de relaciones \"poscionales\" que dependen del número de cabezas (si quiero que apredan más relaciones posicionales -> Más cabezas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zna3md12rtMf"
      },
      "source": [
        "Y para terminar hay N modulos (en el paper de 2017, 6 por Encoder y 6 por Decoder) apilados, ¿por qué? Aquí una idea intuitiva es la misma que en las convolucionales, para que pueda memorizar relaciones posicionales más complicadas (y tener más memoria entrenable)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3ar9UqqrtMf"
      },
      "source": [
        "## Large Language Models (Pretrained Transformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt8wIErgrtMf"
      },
      "source": [
        "Y en 2018, llegaron las arquitecturas que apilaban multihead attention layers pero ya olvidándose de decoders y encoders... Una sola columna con muchos módulos... pero con la magia de estar pre-entrenadas para una tarea (generalmente adivina cuál es la siguiente palabra de la sentencia (GPT) o de cada sentencia te voy a ocultar 2 palabras, adivina cuales son (BERT)).\n",
        "\n",
        "En esencia, están memorizando todas las relaciones que existen entre las palabras de un lenguaje... Por eso cada vez se hacen más grandes (para memorizar más) y tienen más parámetros... y necesitan más que les des de comer (si quieres tenerlo todo representado)... (Imagina que le pudieras dar para entrenar a un red convolucional todas las imágenes posibles que existen)\n",
        "\n",
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/gpt2_vs_BERT.jpg?raw=1\" alt=\"Multihead Attention Layer\" width=\"800\" />\n",
        "\n",
        "Luego estos modelos preentrenados se adaptan (fine-tunning), o sea se hace transfer learning, a otros tipos de problemas (clasificación, sentiment analysis, question and answers, y ahora chats, texto generativo)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOi7vkFOAClz"
      },
      "source": [
        "## Instruct LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_8qCUXkAE_r"
      },
      "source": [
        "Hoy en día, lo que realmente se ha puesto \"de moda\" no es emplear los LLM como los vistos hasta ahora, sino los fine-tuned y más concretamente los basados en una aproximación denominada \"Instruct LLM\" (partiendo de un paper de OpenAI):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yY38EYRAbDw"
      },
      "source": [
        "<img src=\"https://github.com/Jaimegrp/Practicando/blob/main/Texto/img/rlhf.jpg?raw=1\" alt=\"Instruct LLM\" width=\"800\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrzskW7CrtMg"
      },
      "source": [
        "Han surgido decenas de modelos, propietarios y abiertos, los más destacados (que puedes usar):  \n",
        "[OpenAI: ChatGPT](https://chat.openai.com/)  \n",
        "[Google: Gemini](https://gemini.google.com/?hl=es)  \n",
        "[Mistral: Mistral MoE](https://mistral.ai/)  \n",
        "[Meta: Llama](https://www.llama.com/)  \n",
        "[Hugging Face: miles y miles de modelos](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)  \n",
        "[Preplexity AI: combina GPT-3.5 y su LLM propietario](https://www.perplexity.ai/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qhj4QK6rtMg"
      },
      "source": [
        "### Bonus: PROGRAMANDO UN TRANSFORMER (GPT-2 like)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZwnnhSVrtMh"
      },
      "source": [
        "En las siguientes celdas se muestra como construir un transformer para nuestro traductor... para el que quiera jugar (extraido del Hands-On...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL5UExD9rtMh"
      },
      "source": [
        "Positional encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrOa1E-Qx7la"
      },
      "outputs": [],
      "source": [
        "tf.shape()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2TZgJSZrtMi"
      },
      "outputs": [],
      "source": [
        "max_length = 50  # max length in the whole training set\n",
        "embed_size = 128\n",
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "pos_embed_layer = tf.keras.layers.Embedding(max_length, embed_size)\n",
        "batch_max_len_enc = tf.shape(encoder_embeddings)[1]\n",
        "encoder_in = encoder_embeddings + pos_embed_layer(tf.range(batch_max_len_enc))\n",
        "batch_max_len_dec = tf.shape(decoder_embeddings)[1]\n",
        "decoder_in = decoder_embeddings + pos_embed_layer(tf.range(batch_max_len_dec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwLLeq-TrtMi"
      },
      "source": [
        "Alternatively, we can use fixed, non-trainable positional encodings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hjsMZ3xrtMi"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        assert embed_size % 2 == 0, \"embed_size must be even\"\n",
        "        p, i = np.meshgrid(np.arange(max_length),\n",
        "                           2 * np.arange(embed_size // 2))\n",
        "        pos_emb = np.empty((1, max_length, embed_size))\n",
        "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
        "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
        "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_max_length = tf.shape(inputs)[1]\n",
        "        return inputs + self.pos_encodings[:, :batch_max_length]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vArtPEfzrtMi"
      },
      "outputs": [],
      "source": [
        "pos_embed_layer = PositionalEncoding(max_length, embed_size)\n",
        "encoder_in = pos_embed_layer(encoder_embeddings)\n",
        "decoder_in = pos_embed_layer(decoder_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkyBTS9HrtMi"
      },
      "outputs": [],
      "source": [
        "# extra code – this cells generates and saves Figure 16–9\n",
        "figure_max_length = 201\n",
        "figure_embed_size = 512\n",
        "pos_emb = PositionalEncoding(figure_max_length, figure_embed_size)\n",
        "zeros = np.zeros((1, figure_max_length, figure_embed_size), np.float32)\n",
        "P = pos_emb(zeros)[0].numpy()\n",
        "i1, i2, crop_i = 100, 101, 150\n",
        "p1, p2, p3 = 22, 60, 35\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(9, 5))\n",
        "ax1.plot([p1, p1], [-1, 1], \"k--\", label=\"$p = {}$\".format(p1))\n",
        "ax1.plot([p2, p2], [-1, 1], \"k--\", label=\"$p = {}$\".format(p2), alpha=0.5)\n",
        "ax1.plot(p3, P[p3, i1], \"bx\", label=\"$p = {}$\".format(p3))\n",
        "ax1.plot(P[:,i1], \"b-\", label=\"$i = {}$\".format(i1))\n",
        "ax1.plot(P[:,i2], \"r-\", label=\"$i = {}$\".format(i2))\n",
        "ax1.plot([p1, p2], [P[p1, i1], P[p2, i1]], \"bo\")\n",
        "ax1.plot([p1, p2], [P[p1, i2], P[p2, i2]], \"ro\")\n",
        "ax1.legend(loc=\"center right\", fontsize=14, framealpha=0.95)\n",
        "ax1.set_ylabel(\"$P_{(p,i)}$\", rotation=0, fontsize=16)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.hlines(0, 0, figure_max_length - 1, color=\"k\", linewidth=1, alpha=0.3)\n",
        "ax1.axis([0, figure_max_length - 1, -1, 1])\n",
        "ax2.imshow(P.T[:crop_i], cmap=\"gray\", interpolation=\"bilinear\", aspect=\"auto\")\n",
        "ax2.hlines(i1, 0, figure_max_length - 1, color=\"b\", linewidth=3)\n",
        "cheat = 2  # need to raise the red line a bit, or else it hides the blue one\n",
        "ax2.hlines(i2+cheat, 0, figure_max_length - 1, color=\"r\", linewidth=3)\n",
        "ax2.plot([p1, p1], [0, crop_i], \"k--\")\n",
        "ax2.plot([p2, p2], [0, crop_i], \"k--\", alpha=0.5)\n",
        "ax2.plot([p1, p2], [i2+cheat, i2+cheat], \"ro\")\n",
        "ax2.plot([p1, p2], [i1, i1], \"bo\")\n",
        "ax2.axis([0, figure_max_length - 1, 0, crop_i])\n",
        "ax2.set_xlabel(\"$p$\", fontsize=16)\n",
        "ax2.set_ylabel(\"$i$\", rotation=0, fontsize=16)\n",
        "save_fig(\"positional_embedding_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP0A5FmgrtMj"
      },
      "source": [
        "### Multi-Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaNUlZBNrtMj"
      },
      "outputs": [],
      "source": [
        "N = 2  # instead of 6\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "n_units = 128  # for the first Dense layer in each Feed Forward block\n",
        "encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\n",
        "Z = encoder_in\n",
        "for _ in range(N):\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
        "    Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
        "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
        "    Z = tf.keras.layers.Dropout(dropout_rate)(Z)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZtmcl8lrtMj"
      },
      "outputs": [],
      "source": [
        "encoder_outputs = Z  # let's save the encoder's final outputs\n",
        "Z = decoder_in  # the decoder starts with its own inputs\n",
        "for _ in range(N):\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
        "    Z = attn_layer(Z, value=Z, attention_mask=causal_mask & decoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
        "    Z = attn_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
        "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQW4XffvrtMj"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly 2 or 3 hours if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmC-BfkgrtMj"
      },
      "outputs": [],
      "source": [
        "Y_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(Z)\n",
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThoiaaMvrtMj"
      },
      "outputs": [],
      "source": [
        "translate(\"I like soccer and also going to the beach\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXaMFZc8rtMj"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "tf_gpu_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
